Timestamp,Model name,Paper title,Bibtex reference,citeCommand,Year Published,Generative model,Trained on code,Reference to Permissive code,Custom Github scrape,Dataset name,Programming languages,Description of dataset (copy from paper),Dataset available,Trained on file level code,Notes
12/22/2023 14:47:11,Codex,Evaluating Large Language Models Trained on Code,"@article{chen2021evaluating,   title={Evaluating large language models trained on code},   author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},   journal={arXiv preprint arXiv:2107.03374},   year={2021} }",\cite{chen2021evaluating},2021,Yes,Yes,No,Yes,,Python,"Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered
out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.",No,Yes,
12/22/2023 15:13:10,CodeT5,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,"@inproceedings{wang2021codet5,     title = ""{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"",     author = ""Wang, Yue  and       Wang, Weishi  and       Joty, Shafiq  and       Hoi, Steven C.H."",     editor = ""Moens, Marie-Francine  and       Huang, Xuanjing  and       Specia, Lucia  and       Yih, Scott Wen-tau"",     booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",     month = nov,     year = ""2021"",     address = ""Online and Punta Cana, Dominican Republic"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2021.emnlp-main.685"",     doi = ""10.18653/v1/2021.emnlp-main.685"",     pages = ""8696--8708"",     abstract = ""Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at \url{https://github.com/salesforce/CodeT5}."", }",\cite{wang2021codet5},2021,Yes,Yes,No,No,"CodeSearchNet, BigQuery","Ruby, JS, Go, Python, Java, PHP, C, CSharp","We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from BigQuery to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ  round 8.35 million instances for pretraining. Table 1 shows some basic statistics. To obtain the identifier labels from code, we leverage the tree-sitter to convert the PL into an abstract
syntax tree and then extract its node type information. We filter out reserved keywords for each PL from its identifier list. We observe that PLs have different identifier rates, where Go has the least rate of 19% and Ruby has the highest rate of 32%.",Yes,Yes,
12/22/2023 15:26:24,CodeGen,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,"@inproceedings{ nijkamp2023codegen, title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis}, author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=iaYcJKpY2B_} }",\cite{ nijkamp2023codegen},2023,Yes,Yes,Yes,No,"ThePile, BigQuery, BigPython","C, CPP, Go, Java, JS, Python","The family of CODEGEN models is trained sequentially on three datasets: THEPILE, BIGQUERY, and BIGPYTHON.
The natural language dataset THEPILE is an 825.18 GiB English text corpus collected by Gao et al. (2020) for language modeling (MIT license). The dataset is constructed from 22 diverse high-quality subsets, one of which is programming language data collected from GitHub repositories with >100 stars that constitute 7.6% of the dataset. Since the majority of THEPILE is English text, the resulting models are called as natural language CODEGEN models (CODEGEN-NL).
The multi-lingual dataset BIGQUERY is a subset of Google’s publicly available BigQuery dataset, which consists of code (under open-source license) in multiple programming languages. For the multilingual training, the following 6 programming languages are chosen: C, C++, Go, Java, JavaScript, and Python. Thus, we refer to models trained on the BIGQUERY as multi-lingual CODEGEN models (CODEGEN-MULTI).
The mono-lingual dataset BIGPYTHON contains a large amount of data in the programming language, Python. We have compiled public, non-personal information from GitHub consisting of permissively
licensed Python code in October 2021. Consequently, we refer to models trained on BIGPYTHON as mono-lingual CODEGEN models (CODEGEN-MONO).
The pre-processing follows: (1) filtering, (2) deduplication, (3) tokenization, (4) shuffling, and
(5) concatenation. For details on THEPILE, we refer to Gao et al. (2020). For BIGQUERY and
BIGPYTHON, we refer to Appendix A. Table 5 summarizes the statistics of the training corpora.",Yes,Yes,
12/22/2023 15:33:54,GPT-3,Language Models are Few-Shot Learners,"@article{brown2020language,   title={Language models are few-shot learners},   author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},   journal={Advances in neural information processing systems},   volume={33},   pages={1877--1901},   year={2020} }",\cite{brown2020language},2020,Yes,No,,,,,,,,
12/22/2023 15:40:21,Incoder,INCODER: A GENERATIVE MODEL FOR CODE INFILLING AND SYNTHESIS,"@inproceedings{ fried2023incoder, title={InCoder: A Generative Model for Code Infilling and Synthesis}, author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Scott Yih and Luke Zettlemoyer and Mike Lewis}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=hQwb-lbM6EL} }",\cite{fried2023incoder},2023,Yes,Yes,Yes,Yes,,"Python + 28 others (JS, HTML, C, CPP, Java, Jupyter notebook, TS, Go, CSS, PHP, PLSQL, CSharp, ruby, SQL, shell, Objective-C, perl, scala, rust","To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments. Our primary focus in this paper is on the Python language, but we also include code files from 28 total languages and StackOverflow content from all available languages. We decontaminate our pre-training corpus by removing all datasets which we use in our evaluation experiments. See Section A.1 for details. Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it in Python, and a total of 57 GB of content from StackOverflow. See Figure 3 for size by language.",No,Yes,
12/22/2023 15:45:49,T5,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"@article{raffel2020exploring,   title={Exploring the limits of transfer learning with a unified text-to-text transformer},   author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},   journal={The Journal of Machine Learning Research},   volume={21},   number={1},   pages={5485--5551},   year={2020},   publisher={JMLRORG} }",\cite{raffel2020exploring},2020,Yes,No,,,,,,,,
12/22/2023 15:55:14,GPT-2,Language Models are Unsupervised Multitask Learners,"@inproceedings{Radford2019LanguageMA,   title={Language Models are Unsupervised Multitask Learners},   author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},   year={2019},   url={https://api.semanticscholar.org/CorpusID:160025533} }",\cite{Radford2019LanguageMA},2019,Yes,No,,,,,,,,
12/22/2023 16:13:37,PLBART,Unified Pre-training for Program Understanding and Generation,"@inproceedings{ahmad2021unified,   title={Unified Pre-training for Program Understanding and Generation},   author={Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},   booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},   pages={2655--2668},   year={2021} }",\cite{ahmad2021unified},2021,Yes,Yes,,,,,,,No,
12/22/2023 16:16:20,BART,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","@inproceedings{lewis-etal-2020-bart,     title = ""{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"",     author = ""Lewis, Mike  and       Liu, Yinhan  and       Goyal, Naman  and       Ghazvininejad, Marjan  and       Mohamed, Abdelrahman  and       Levy, Omer  and       Stoyanov, Veselin  and       Zettlemoyer, Luke"",     editor = ""Jurafsky, Dan  and       Chai, Joyce  and       Schluter, Natalie  and       Tetreault, Joel"",     booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",     month = jul,     year = ""2020"",     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2020.acl-main.703"",     doi = ""10.18653/v1/2020.acl-main.703"",     pages = ""7871--7880"",     abstract = ""We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."", }",\cite{lewis-etal-2020-bart},2020,Yes,No,,,,,,,,
12/22/2023 16:24:59,mT5,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"@inproceedings{xue2021mt5,   title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},   author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},   booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},   pages={483--498},   year={2021} }",\cite{xue2021mt5},2021,Yes,No,,,,,,,,
12/27/2023 10:27:44,CPM-2,CPM-2: Large-scale Cost-effective Pre-trained Language Models,"@misc{zhang2021cpm2,       title={CPM-2: Large-scale Cost-effective Pre-trained Language Models},        author={Zhengyan Zhang and Yuxian Gu and Xu Han and Shengqi Chen and Chaojun Xiao and Zhenbo Sun and Yuan Yao and Fanchao Qi and Jian Guan and Pei Ke and Yanzheng Cai and Guoyang Zeng and Zhixing Tan and Zhiyuan Liu and Minlie Huang and Wentao Han and Yang Liu and Xiaoyan Zhu and Maosong Sun},       year={2021},       eprint={2106.10715},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{zhang2021cpm2},2021,Yes,No,,,,,,,,
12/27/2023 11:29:06,PanGu-α,PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,"@article{weizeng,   author       = {Wei Zeng and                   Xiaozhe Ren and                   Teng Su and                   Hui Wang and                   Yi Liao and                   Zhiwei Wang and                   Xin Jiang and                   ZhenZhang Yang and                   Kaisheng Wang and                   Xiaoda Zhang and                   Chen Li and                   Ziyan Gong and                   Yifan Yao and                   Xinjing Huang and                   Jun Wang and                   Jianfeng Yu and                   Qi Guo and                   Yue Yu and                   Yan Zhang and                   Jin Wang and                   Hengtao Tao and                   Dasen Yan and                   Zexuan Yi and                   Fang Peng and                   Fangqing Jiang and                   Han Zhang and                   Lingfeng Deng and                   Yehong Zhang and                   Zhe Lin and                   Chao Zhang and                   Shaojie Zhang and                   Mingyue Guo and                   Shanzhi Gu and                   Gaojun Fan and                   Yaowei Wang and                   Xuefeng Jin and                   Qun Liu and                   Yonghong Tian},   title        = {PanGu-{\(\alpha\)}: Large-scale Autoregressive Pretrained Chinese                   Language Models with Auto-parallel Computation},   journal      = {CoRR},   volume       = {abs/2104.12369},   year         = {2021},   url          = {https://arxiv.org/abs/2104.12369},   eprinttype    = {arXiv},   eprint       = {2104.12369},   timestamp    = {Tue, 31 Oct 2023 15:43:37 +0100},   biburl       = {https://dblp.org/rec/journals/corr/abs-2104-12369.bib},   bibsource    = {dblp computer science bibliography, https://dblp.org} }",\cite{weizeng},2021,Yes,No,,,,,,,,
12/27/2023 11:44:18,T0,Multitask Prompted Training Enables Zero-Shot Task Generalization,"@misc{sanh2022multitask,       title={Multitask Prompted Training Enables Zero-Shot Task Generalization},        author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},       year={2022},       eprint={2110.08207},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{sanh2022multitask},2022,Yes,No,,,,,,,,
12/27/2023 12:24:57,GPT-NeoX,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,"@inproceedings{black-etal-2022-gpt,     title = ""{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model"",     author = ""Black, Sidney  and       Biderman, Stella  and       Hallahan, Eric  and       Anthony, Quentin  and       Gao, Leo  and       Golding, Laurence  and       He, Horace  and       Leahy, Connor  and       McDonell, Kyle  and       Phang, Jason  and       Pieler, Michael  and       Prashanth, Usvsn Sai  and       Purohit, Shivanshu  and       Reynolds, Laria  and       Tow, Jonathan  and       Wang, Ben  and       Weinbach, Samuel"",     editor = ""Fan, Angela  and       Ilic, Suzana  and       Wolf, Thomas  and       Gall{\'e}, Matthias"",     booktitle = ""Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models"",     month = may,     year = ""2022"",     address = ""virtual+Dublin"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.bigscience-1.9"",     doi = ""10.18653/v1/2022.bigscience-1.9"",     pages = ""95--136"", }",\cite{black-etal-2022-gpt},2022,Yes,Yes,No,No,ThePile-GitHub,"Assembly', 'Batchfile', 'C', 'C#', 'C++', 'CMake', 'COBOL', 'CSS', 'CSV', 'Clojure', 'CoffeeScript', 'DM', 'Dart', 'Dockerfile', 'Elixir', 'Erlang', 'Fortran', 'Go', 'Groovy', 'HTML', 'Haskell', 'INI', 'JSON', 'Java', 'JavaScript', 'Julia', 'Kotlin', 'Lisp', 'Lua', 'Makefile', 'Markdown', 'Matlab', 'None', 'OCaml', 'Objective-C', 'PHP', 'Pascal', 'Perl', 'PowerShell', 'Prolog', 'Python', 'R', 'Ruby', 'Rust', 'SQL', 'Scala', 'Shell', 'Swift', 'TOML', 'TeX', 'TypeScript', 'Verilog', 'Visual Basic', 'XML', 'YAML'","""GPT-NeoX-20B was trained on the Pile (Gao et al.,2020), a massive curated dataset designed specifically for training large language models. It consists of data from 22 data sources, coarsely broken down into 5 categories: 
Academic Writing: Pubmed Abstracts and PubMed Central, arXiv, FreeLaw, USPTO Backgrounds, PhilPapers, NIH Exporter;
Web-scrapes and Internet Resources: CommonCrawl, OpenWebText, StackExchange, Wikipedia (English); 
Prose: BookCorpus2, Bibliotik, Project
Gutenberg (PG-19; Rae et al., 2019);
Dialogue: Youtube subtitles, Ubuntu IRC, OpenSubtitles (Lison and Tiedemann, 2016),
Hacker News, EuroParl (Koehn, 2005);
Miscellaneous: GitHub, the DeepMind Mathematics dataset (Saxton et al., 2019), Enron
Emails (Klimt and Yang, 2004);
In aggregate, the Pile consists of over 825GiB of raw text data. The diverse data sources reflects our desire for a general-purpose language model.
",Yes,Yes,
12/27/2023 12:51:17,Tk-instruct,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,"@inproceedings{wang-etal-2022-super,     title = ""Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks"",     author = ""Wang, Yizhong  and       Mishra, Swaroop  and       Alipoormolabashi, Pegah  and       Kordi, Yeganeh  and       Mirzaei, Amirreza  and       Naik, Atharva  and       Ashok, Arjun  and       Dhanasekaran, Arut Selvan  and       Arunkumar, Anjana  and       Stap, David  and       Pathak, Eshaan  and       Karamanolakis, Giannis  and       Lai, Haizhi  and       Purohit, Ishan  and       Mondal, Ishani  and       Anderson, Jacob  and       Kuznia, Kirby  and       Doshi, Krima  and       Pal, Kuntal Kumar  and       Patel, Maitreya  and       Moradshahi, Mehrad  and       Parmar, Mihir  and       Purohit, Mirali  and       Varshney, Neeraj  and       Kaza, Phani Rohitha  and       Verma, Pulkit  and       Puri, Ravsehaj Singh  and       Karia, Rushang  and       Doshi, Savan  and       Sampat, Shailaja Keyur  and       Mishra, Siddhartha  and       Reddy A, Sujan  and       Patro, Sumanta  and       Dixit, Tanay  and       Shen, Xudong"",     editor = ""Goldberg, Yoav  and       Kozareva, Zornitsa  and       Zhang, Yue"",     booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.emnlp-main.340"",     doi = ""10.18653/v1/2022.emnlp-main.340"",     pages = ""5085--5109"",}",\cite{wang-etal-2022-super},2022,Yes,Yes,,,,,,,No,"This one is trained on a HUGE dataset with a HUGE amount of tasks. Most of them are text-based tasks, but it also includes text-to-code, code-to-text and program execution tasks, so that is why I said it was trained on code, but that represents a small amount. "
12/28/2023 11:36:15,UL2,UL2: Unifying Language Learning Paradigms,"@article{tay2022unifying,   title={Unifying language learning paradigms},   author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},   journal={arXiv preprint arXiv:2205.05131},   year={2022} }",\cite{tay2022unifying},2022,Yes,No,,,,,,,,Maybe in the flan?
12/28/2023 11:41:58,OPT,OPT: Open Pre-trained Transformer Language Models,"@article{zhang2022opt,   title={Opt: Open pre-trained transformer language models},   author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},   journal={arXiv preprint arXiv:2205.01068},   year={2022} }",\cite{zhang2022opt},2022,Yes,No,,,,,,,,
12/28/2023 11:45:25,NLLB,No Language Left Behind: Scaling Human-Centered Machine Translation,"@article{costa2022no,   title={No language left behind: Scaling human-centered machine translation},   author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},   journal={arXiv preprint arXiv:2207.04672},   year={2022} }",\cite{costa2022no},2022,Yes,No,,,,,,,,
12/28/2023 11:56:03,CodeGeeX,CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X,"@article{zheng2023codegeex,   title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},   author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},   journal={arXiv preprint arXiv:2303.17568},   year={2023} }",\cite{zheng2023codegeex},2023,Yes,Yes,No,Yes,"The Pils, CodeParrot, custom","CPP, C, CSharp, Cuda, Objective-C, Objective-CPP, Python, Java, Scala, Tex, HTML, Php, JS, TS, Go, Shell, Rust, CSS, SQL, Kotlin, Pascal, R, Fortran","Code Corpus. The training corpus contains two parts. The ﬁrst part is from open source code datasets, the Pile (Gao et al., 2020) and CodeParrot6. The Pile contains a subset of public repositories with more than 100 stars on GitHub, from which we select ﬁles of 23 popular programming languages including C++, Python, Java, JavaScript, C, Go, and so on. We identify the programming language of each ﬁle based on its sufﬁx and the major language of the repository it belongs to. CodeParrot is another public Python dataset from BigQuery. The second part is supplementary data of Python, Java, and C++ directly scraped from GitHub public repositories that do not appear in the ﬁrst part. We choose repositories that have at least one star and a total size within 10MB, then we ﬁlter out ﬁles that: 1) have more than 100 characters per line on average, 2) are automatically generated, 3) have a
ratio of alphabet less than 40%, 4) are bigger than 100KB or smaller than 1KB. We format Python code according to the PEP8 standards. 
Figure 3 shows the composition of the 158B-token training data, containing 23 programming languages. We divide the training data into segments of equal length. To help the model distinguish between multiple languages, we add a language-speciﬁc tag before each segment in the form of [Comment sign]language: [LANG], e.g., # language: Python.",Yes,Yes,
12/28/2023 12:06:01,GLM,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,"@inproceedings{du2022glm,   title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},   author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},   booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},   pages={320--335},   year={2022} }",\cite{du2022glm},2022,Yes,No,,,,,,,,
12/28/2023 12:27:55,FLAN-T5,Scaling Instruction-Finetuned Language Models,"@article{chung2022scaling,   title={Scaling instruction-finetuned language models},   author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},   journal={arXiv preprint arXiv:2210.11416},   year={2022} }",\cite{chung2022scaling},2022,Yes,Yes,Yes,No,The Stack,not specified,"ask mixtures. Prior literature has shown that increasing the number of tasks in ﬁnetuning with instructions improves generalization to unseen tasks (Wei et al., 2021; Sanh et al., 2021, inter alia). In this paper we scale to 1,836 ﬁnetuning tasks by combining four mixtures from prior work: Muﬃn, T0-SF, NIV2, and CoT, as summarized in Figure 2. Muﬃn3 (80 tasks) comprises 62 tasks from Wei et al. (2021) and 26 new tasks that we added in this work, including dialog data (Byrne et al., 2019; Anantha et al., 2021; Dai et al., 2022) and program synthesis data (Yasunaga and Liang, 2020; Li et al., 2022). T0-SF (193 tasks) comprises tasks from T0 (Sanh et al., 2021) that do not overlap with the data used in Muﬃn (SF stands for “sans Flan”). NIV2 (1554 tasks) comprises tasks from Wang et al. (2022c).


Muffin contains the stack",Yes,Yes,
12/28/2023 12:45:58,BLOOM,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,"@article{workshop2022bloom,   title={Bloom: A 176b-parameter open-access multilingual language model},   author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},   journal={arXiv preprint arXiv:2211.05100},   year={2022} }",\cite{workshop2022bloom},2022,Yes,Yes,No,No,"ROOTS, BigQuery","C++, C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby, Rust, Scala, and TypeScript","BLOOM was trained on the ROOTS corpus (Lauren¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages.

GitHub Code: The catalogue was further complemented with a dataset of programming
languages collected from the GitHub data collection on Google’s BigQuery,10 which was
then deduplicated of exact matches. The choice of languages to include mirrored the design
choices introduced by Li et al. (2022) to train the AlphaCode model.",Yes,Yes,
12/28/2023 14:24:54,Galactica,Galactica: A Large Language Model for Science,"@article{taylor2022galactica,   title={Galactica: A large language model for science},   author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},   journal={arXiv preprint arXiv:2211.09085},   year={2022} }",\cite{taylor2022galactica,2022,Yes,Yes,No,Yes,,,"We source academic GitHub repositories from the Papers with Code index for machine learning, physics, mathematics, statistics and astronomy. The index does not explicitly cover sciences such as biology and chemistry, but many of these repositories are captured as part of the general machine learning index. We exclude repositories that do not have a license or copyright file.",No,Yes,
12/28/2023 15:05:28,OPT-IML,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,"@article{iyer2022opt,   title={Opt-iml: Scaling language model instruction meta learning through the lens of generalization},   author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},   journal={arXiv preprint arXiv:2212.12017},   year={2022} }",\cite{iyer2022opt},2022,Yes,No,,,,,,,,
12/28/2023 15:08:28,llama,LLaMA: Open and Efficient Foundation Language Models,"@article{touvron2023llama,   title={Llama: Open and efficient foundation language models},   author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},   journal={arXiv preprint arXiv:2302.13971},   year={2023} }",\cite{touvron2023llama},2023,Yes,Yes,Yes,No,BigQuery,,"Github [4.5%]. We use the public GitHub dataset available on Google BigQuery. We only kept projects that are distributed under the Apache, BSD and MIT licenses. Additionally, we filtered low quality files with heuristics based on the line length or proportion of  alphanumeric characters, and removed boilerplate, such as headers, with regular expressions. Finally, we deduplicate the resulting dataset at the file level, with exact matches.",Yes,Yes,
12/28/2023 15:14:35,Baichuan 2,Baichuan 2: Open Large-scale Language Models,"@article{yang2023baichuan,   title={Baichuan 2: Open large-scale language models},   author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},   journal={arXiv preprint arXiv:2309.10305},   year={2023} }",\cite{yang2023baichuan},2023,Yes,Yes,No,Yes,,,"Data sourcing: During data acquisition, our objective is to pursue comprehensive data
scalability and representativeness. We gather data from diverse sources including general internet webpages, books, research papers, codebases, and more to build an extensive world knowledge system. The composition of the training corpus is shown in Figure 1.",No,Yes,
12/28/2023 15:20:23,QWEN,Qwen Technical Report,"@article{bai2023qwen,   title={Qwen technical report},   author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},   journal={arXiv preprint arXiv:2309.16609},   year={2023} }",\cite{bai2023qwen},2023,Yes,Yes,No,Yes,,,"We believe that relying solely on code data for pretraining can result in a significant loss of the ability to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining on code data (Li et al., 2022; 2023d), we take a different approach (Rozi `ere et al., 2023) by starting with our base models QWEN trained on a combination of text and code data, and then continuing to pretrain on the code data. We continue to pretrain the models on a total of around 90 billion toke",No,Yes,
12/28/2023 15:23:36,FLM,FLM-101B: An Open LLM and How to Train It with $100K Budget,"@article{li2023flm,   title={Flm-101b: An open llm and how to train it with $100 k budget},   author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others},   journal={arXiv preprint arXiv:2309.03852},   year={2023} }",\cite{li2023flm},2023,Yes,No,,,,,,,,
12/28/2023 15:29:46,Skywork,Skywork: A More Open Bilingual Foundation Model,"@article{wei2023skywork,   title={Skywork: A more open bilingual foundation model},   author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\""u}, Weiwei and Hu, Rui and others},   journal={arXiv preprint arXiv:2310.19341},   year={2023} }",\cite{wei2023skywork},2023,Yes,Yes,No,Yes,SkyPile,,"As for Github content, we employ an approach that is similar to (Together Computer, 2023). We have devised a collection of straightforward yet efficacious heuristics, encompassing criteria such as line length filtration and alphanumeric thresholds, designed to discern and exclude content of low quality. Our criteria are specifically oriented toward enhancing
content quality, as opposed to merely curbing its volume. Notably, in contrast to prevailing
practices that involve the wholesale removal of a significant portion of json, xml, yaml, and
html content, we have made a deliberate choice to retain a judiciously proportionate representation of these data formats.",No,Yes,"Only subset of dataset released, doesn t contain code!"
12/28/2023 15:33:27,Pythia,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,"@inproceedings{biderman2023pythia,   title={Pythia: A suite for analyzing large language models across training and scaling},   author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},   booktitle={International Conference on Machine Learning},   pages={2397--2430},   year={2023},   organization={PMLR} }",\cite{biderman2023pythia},2023,Yes,Yes,No,No,The Pile,,"We train 8 model sizes each on both the Pile (Gao et al., 2020; Biderman et al., 2022) and the Pile after deduplication, providing 2 copies of the suite which can be compared.",Yes,Yes,
12/28/2023 15:41:36,codeGen2,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,"@article{nijkamp2023codegen2,   title={Codegen2: Lessons for training llms on programming and natural languages},   author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},   journal={arXiv preprint arXiv:2305.02309},   year={2023} }",\cite{nijkamp2023codegen2},2023,Yes,Yes,Yes,No,"The Pile, The Stack, Big Python",,"We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and refer to them as CodeGen2.1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered with a stronger permissive license guideline, is used.

We test the hypothesis by training a causal decoder with the combined causal language modeling and span corruption objective on a mixture of natural and programming languages (Mix). Examples in a batch are sampled from the Pile (Gao et al., 2020) and the Stack data equally likely

To evaluate this hypothesis, we first train a Prefix-LM on BigPython (Nijkamp et al., 2022) with causal language modeling and compare the model performance on HumanEval against a causal decoder baseline trained for the same number of steps",Yes,Yes,
12/28/2023 15:43:57,StarCoder,StarCoder: may the source be with you!,"@article{li2023starcoder,   title={StarCoder: may the source be with you!},   author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},   journal={arXiv preprint arXiv:2305.06161},   year={2023} }",\cite{li2023starcoder},2023,Yes,Yes,Yes,No,"The Stack v1,2",,"This section describes how we processed the training data of StarCoderBase. We restrict the training set to The Stack v1.2 (Kocetkov et al., 2022), which exclusively contains data from permissively licensed4 GitHub repositories. At the time of the data processing, 44 people opted out of The Stack.",Yes,Yes,
12/28/2023 15:52:26,GShard,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,"@inproceedings{ lepikhin2021gshard, title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=qrwe7XHTmYb} }",\cite{lepikhin2021gshard},2021,Yes,No,,,,,,,,
12/28/2023 15:55:56,Ernie 3.0,ERNIE 3.0: LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION,"@article{sun2021ernie,   title={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},   author={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},   journal={arXiv preprint arXiv:2107.02137},   year={2021} }",\cite{sun2021ernie},2021,Yes,No,,,,,,,,
12/28/2023 16:58:58,Jurassic-1,JURASSIC-1: TECHNICAL DETAILS AND EVALUATION,"@article{lieber2021jurassic,   title={Jurassic-1: Technical details and evaluation},   author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},   journal={White Paper. AI21 Labs},   volume={1},   year={2021} }",\cite{lieber2021jurassic},2021,Yes,Yes,No,No,The Pile,,"We tested on a variety of domains found in the Pile dataset (Gao et al., 2020), including web (Pile’s Common Crawl corpus), academic text formatted in LaTeX (arXiv), fiction books (Books3, Gutenberg), computer programs (GitHub), and more. In all but three corpora, our Jurassic-1 models outperform their GPT-3 counterparts. See complete results in Table. 4",Yes,Yes,"They evaluate and have a lot of data on it, but never say that they trained on it"
12/28/2023 17:04:12,HyperClova,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,"@article{kim2021changes,   title={What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers},   author={Kim, Boseop and Kim, HyoungSeok and Lee, Sang-Woo and Lee, Gichang and Kwak, Donghyun and Jeon, Dong Hyeon and Park, Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and others},   journal={arXiv preprint arXiv:2109.04650},   year={2021} }",\cite{kim2021changes},2021,Yes,No,,,,,,,,
12/28/2023 17:10:26,Yuan 1.0,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,"@article{wu2021yuan,   title={Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning},   author={Wu, Shaohua and Zhao, Xudong and Yu, Tong and Zhang, Rongguo and Shen, Chong and Liu, Hongli and Li, Feng and Zhu, Hong and Luo, Jiangang and Xu, Liang and others},   journal={arXiv preprint arXiv:2110.04725},   year={2021} }",\cite{wu2021yuan},2021,Yes,No,,,,,,,,
12/28/2023 17:21:49,FLAN,Finetuned Language Models are Zero-Shot Learners ,"@inproceedings{ wei2022finetuned, title={Finetuned Language Models are Zero-Shot Learners}, author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le}, booktitle={International Conference on Learning Representations}, year={2022}, url={https://openreview.net/forum?id=gEZrGCozdqR} }",\cite{wei2022finetuned},2022,Yes,Yes,,,,,,,No,
12/28/2023 17:25:55,WebGPT,WebGPT: Browser-assisted question-answering with human feedback,"@article{nakano2021webgpt,   title={Webgpt: Browser-assisted question-answering with human feedback},   author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},   journal={arXiv preprint arXiv:2112.09332},   year={2021} }",\cite{nakano2021webgpt},2021,Yes,No,,,,,,,,
12/28/2023 17:31:07,Gopher,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","@article{rae2021scaling,   title={Scaling language models: Methods, analysis \& insights from training gopher},   author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},   journal={arXiv preprint arXiv:2112.11446},   year={2021} }",\cite{rae2021scaling},2021,Yes,Yes,Yes,Yes,MassiveText,,"For Github, we restrict the data to only include code with the following permissive licenses: Apache License version 2.0, MIT license, The 3-clause BSD license, The 2-clause BSD license, Unlicense, CC0, ISC license, and Artistic License 2.0.",No,Yes,
12/28/2023 17:42:48,ERNIE-Code ,ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages,"@misc{chai2023erniecode,       title={ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages},        author={Yekun Chai and Shuohuan Wang and Chao Pang and Yu Sun and Hao Tian and Hua Wu},       year={2023},       eprint={2212.06742},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{chai2023erniecode},2023,Yes,Yes,,,,,,,No,
12/28/2023 17:59:18,CodeT5Mix,CodeT5Mix: A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation,"@misc{ wang2023codetmix, title={CodeT5Mix: A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation}, author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Junnan Li and Steven Hoi}, year={2023}, url={https://openreview.net/forum?id=VPCi3STZcaO} }",\cite{wang2023codetmix},2023,Yes,Yes,Yes,No,"CodeSearchNet, GitHub Code Dataset (only latter one is file level)","Python, Java, Ruby, JavaScript, Go, PHP, C, C++, C#","We enlarge the pretraining dataset of CodeSearchNet (Husain et al., 2019) with the recently released GitHub Code dataset. We select nine PLs (Python, Java, Ruby, JavaScript, Go, PHP, C, C++, C#) and filter the dataset by preserving only permissively licensed code and files with 50 to 2000 tokens. Besides, we filter out the overlapped subset with CodeSearchNet and other downstream tasks by checking their GitHub repositories.",Yes,Yes,
12/28/2023 18:07:09,CodeRL,CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning,"@misc{le2022coderl,       title={CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},        author={Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven C. H. Hoi},       year={2022},       eprint={2207.01780},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{le2022coderl},2022,Yes,Yes,Yes,No,"CodeSearchNet, Python GitHub Code Dataset (only latter one has file level code)","Go, Java, JavaScript, PHP, Python, Ruby","We enlarge the Python pretraining dataset using the recently released
large-scale Github Code dataset. We have compiled public, non-personal information from GitHub consisting of permissively licensed Python code (e.g. “mit”, “apache-2”, “bsd-3-clause”, “bsd-2- 126 clause”, “cc0-1.0”, “unlicense”, “isc”). The resulting Python dataset (GCPY) has 10.5B tokens and is 10x larger than the CodeSearchNet (CSN) corpus [Husain et al., 2019] used in the original CodeT5 [Wang et al., 2021].",Yes,Yes,
12/28/2023 18:29:51,JuPyT5,Training and Evaluating a Jupyter Notebook Data Science Assistant,"@misc{chandel2022training,       title={Training and Evaluating a Jupyter Notebook Data Science Assistant},        author={Shubham Chandel and Colin B. Clement and Guillermo Serrato and Neel Sundaresan},       year={2022},       eprint={2201.12901},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{chandel2022training},2022,Yes,Yes,No,Yes,Custom scrape,Python (Jupyter Notebooks),"We train our models on all Jupyter notebooks from all the
public GitHub repositories with Jupyter Notebooks as the primary language label as of April 2021, excluding repositories from which our Data Science Problems evaluation were curated. We first discuss the filtering pipeline which yielded 1119 high-quality data science questions with validating unit tests which compose DSP.
",No,Yes,
12/28/2023 18:52:18,PyMT5,PyMT5: multi-mode translation of natural language and Python code with transformers,"@misc{clement2020pymt5,       title={PyMT5: multi-mode translation of natural language and Python code with transformers},        author={Colin B. Clement and Dawn Drain and Jonathan Timcheck and Alexey Svyatkovskiy and Neel Sundaresan},       year={2020},       eprint={2010.03150},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{clement2020pymt5},2020,Yes,Yes,,,,,,,No,"Only takes python methods from files, not entire file content."
12/29/2023 12:33:14,GLaM,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,"@inproceedings{du2022glam,   title={Glam: Efficient scaling of language models with mixture-of-experts},   author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},   booktitle={International Conference on Machine Learning},   pages={5547--5569},   year={2022},   organization={PMLR} }",\cite{du2022glam},2022,Yes,No,,,,,,,,
12/29/2023 12:37:09,LaMDA,LaMDA: Language Models for Dialog Applications,"@article{thoppilan2022lamda,   title={Lamda: Language models for dialog applications},   author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},   journal={arXiv preprint arXiv:2201.08239},   year={2022} }",\cite{thoppilan2022lamda},2022,Yes,Yes,,,,,,,No,
12/29/2023 12:44:48,MT-NLG,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","@article{smith2022using,   title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},   author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},   journal={arXiv preprint arXiv:2201.11990},   year={2022} }",\cite{smith2022using},2022,Yes,Yes,No,No,The Pile,,"We largely built upon prior work described in [9, 17] to generate our training set. First, we selected a subset of the datasets from The Pile that we observed to be of the highest relative quality (see Table 1)",Yes,Yes,
12/29/2023 12:48:30,AlphaCode,Competition-Level Code Generation with AlphaCode,"@article{li2022competition,   title={Competition-level code generation with alphacode},   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},   journal={Science},   volume={378},   number={6624},   pages={1092--1097},   year={2022},   publisher={American Association for the Advancement of Science} }",\cite{li2022competition},2022,Yes,Yes,Yes,Yes,,"C++, C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby, Rust, Scala, and TypeScript","Our pre-training dataset is based on a snapshot of selected public GitHub repositories taken on 2021/07/14. We included all code ﬁles from several popular languages: C++, C#, Go, Java, JavaScript, Lua, PHP, Python, Ruby, Rust, Scala, and TypeScript. Following previous work (Chen et al., 2021), we ﬁltered out all ﬁles larger than 1MB or with lines longer than 1000 characters, to exclude automatically generated code. We also removed duplicates of the same ﬁle, ignoring whitespace in comparisons. After ﬁltering, our ﬁnal pre-training dataset contains a total of 715.1 GB of code. The dataset composition across languages can be found in the appendix (Table A1).",No,Yes,
12/29/2023 12:52:04,InstructGPT,Training language models to follow instructions with human feedback,"@article{ouyang2022training,   title={Training language models to follow instructions with human feedback},   author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},   journal={Advances in Neural Information Processing Systems},   volume={35},   pages={27730--27744},   year={2022} }",\cite{ouyang2022training},2022,Yes,Yes,,,,,,,No,
12/29/2023 12:56:16,Chinchilla,Training Compute-Optimal Large Language Models,"@article{hoffmann2022training,   title={Training compute-optimal large language models},   author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},   journal={arXiv preprint arXiv:2203.15556},   year={2022} }",\cite{hoffmann2022training},2022,Yes,Yes,No,Yes,,,"Disk Size Documents Sampling proportion Epochs in 1.4T tokens
GitHub 3.1 TB 142M 4% (3%) 0.13",No,Yes,
12/29/2023 13:06:08,PaLM,PaLM: Scaling Language Modeling with Pathways,"@article{chowdhery2023palm,   title={Palm: Scaling language modeling with pathways},   author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},   journal={Journal of Machine Learning Research},   volume={24},   number={240},   pages={1--113},   year={2023} }",\cite{chowdhery2023palm},2023,Yes,Yes,Yes,Yes,PaLM dataset,"24 common programming languages, including Java, HTML, Javascript, Python, PHP, C#, XML, C++, and C","The source code in the pretraining dataset is obtained from open source repositories on GitHub. We filtered the files by the license included in the repository; copyleft licenses were excluded. We filter the files by filename extension to restrict to one of 24 common programming languages, including Java, HTML, Javascript, Python, PHP, C#, XML, C++, and C, which results in 196GB of source code. Further, we remove duplicates based on Levenshtein distance between the files because duplicate files are known to be common in source code repositories (Lopes et al., 2017; Allamanis, 2019).",No,Yes,
12/29/2023 13:09:25,AlexaTM,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,"@article{soltan2022alexatm,   title={Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model},   author={Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and others},   journal={arXiv preprint arXiv:2208.01448},   year={2022} }",\cite{soltan2022alexatm},2022,Yes,No,,,,,,,,
12/29/2023 13:47:50,PyCodeGPT,CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation,"@inproceedings{ijcai2022p329,   title     = {CERT: Continual Pre-training on Sketches for Library-oriented Code Generation},   author    = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},   booktitle = {Proceedings of the Thirty-First International Joint Conference on                Artificial Intelligence, {IJCAI-22}},   publisher = {International Joint Conferences on Artificial Intelligence Organization},   editor    = {Lud De Raedt},   pages     = {2369--2375},   year      = {2022},   month     = {7},   note      = {Main Track},   doi       = {10.24963/ijcai.2022/329},   url       = {https://doi.org/10.24963/ijcai.2022/329}, }",\cite{ijcai2022p329},2022,Yes,Yes,No,Yes,,Python,"We first crawl 7.6M repository pages hosted on GitHub. Then we consider the language distribution tags in each page to filter the repositories without Python files. As a result, we obtain 1.2M Python-related repository URLs. With the filtered repository URLs, we download all the contents of each repository from GitHub. Following Codex, we remove files over 1MB, as experienced developers usually avoid creating large source code files to maintain good readability. As a result, we get 60.6M raw Python files under 1MB, with a total size of 330GB. Among these files, we further filter out duplicated files, which has been recognized as an important step by CodeParrot. Finally, the number of unique files is reduced to 13.0M, with a total size of 96GB.",No,Yes,
12/29/2023 13:51:50,Sparrow,Improving alignment of dialogue agents via targeted human judgements,"@article{glaese2022improving,   title={Improving alignment of dialogue agents via targeted human judgements},   author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},   journal={arXiv preprint arXiv:2209.14375},   year={2022} }",\cite{glaese2022improving},2022,Yes,No,,,,,,,,
12/29/2023 14:00:05,WeLM,WeLM: A Well-Read Pre-trained Language Model for Chinese,"@article{su2022welm,   title={Welm: A well-read pre-trained language model for chinese},   author={Su, Hui and Zhou, Xiao and Yu, Houjin and Shen, Xiaoyu and Chen, Yuwen and Zhu, Zilin and Yu, Yang and Zhou, Jie},   journal={arXiv preprint arXiv:2209.10372},   year={2022} }",\cite{su2022welm},2022,Yes,No,,,,,,,,
12/29/2023 14:05:57,U-PaLM,Transcending Scaling Laws with 0.1% Extra Compute,"@article{tay2022transcending,   title={Transcending scaling laws with 0.1\% extra compute},   author={Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q and So, David R and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and others},   journal={arXiv preprint arXiv:2210.11399},   year={2022} }",\cite{tay2022transcending},2022,Yes,Yes,No,No,PaLM,,"To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on additional sources of data (labeled or unlabeled).",No,Yes,
12/29/2023 14:08:18,GPT-CC,,,,2021,Yes,Yes,No,Yes,"Trained on The Pile, custom scrape (fine-tuned on GPT Code Clippy Dataset)","Assembly', 'Batchfile', 'C', 'C#', 'C++', 'CMake', 'COBOL', 'CSS', 'CSV', 'Clojure', 'CoffeeScript', 'DM', 'Dart', 'Dockerfile', 'Elixir', 'Erlang', 'Fortran', 'Go', 'Groovy', 'HTML', 'Haskell', 'INI', 'JSON', 'Java', 'JavaScript', 'Julia', 'Kotlin', 'Lisp', 'Lua', 'Makefile', 'Markdown', 'Matlab', 'None', 'OCaml', 'Objective-C', 'PHP', 'Pascal', 'Perl', 'PowerShell', 'Prolog', 'Python', 'R', 'Ruby', 'Rust', 'SQL', 'Scala', 'Shell', 'Swift', 'TOML', 'TeX', 'TypeScript', 'Verilog', 'Visual Basic', 'XML', 'YAML', ","GPT-Code-Clippy (GPT-CC) is a community effort to create an open-source version of GitHub Copilot, an AI pair programmer based on GPT-3, called GPT-Codex. GPT-CC is fine-tuned on our GPT Code Clippy dataset sourced from publicly available code on GitHub. It was created to allow researchers to easily study large deep learning models that are trained on code to better understand their abilities and limitations. GPT-CC uses the GPT-Neo model as the base language model, which has been pretrained on the Pile dataset and we use the Causal Language Modelling objective to train the model.",Yes,Yes,
12/29/2023 14:48:07,"FIM (multiple models, only considering the ones trained on code)",Efficient Training of Language Models to Fill in the Middle,"@article{Bavarian2022EfficientTO,   title={Efficient Training of Language Models to Fill in the Middle},   author={Mohammad Bavarian and Heewoo Jun and Nikolas A. Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},   journal={ArXiv},   year={2022},   volume={abs/2207.14255},   url={https://api.semanticscholar.org/CorpusID:251135268} }",\cite{Bavarian2022EfficientTO},2022,Yes,Yes,No,Yes,,Python,"We train our code models on the same dataset that was used to train Codex, which is a 159 GB Python dataset scraped in May 2020. As such, we expect no train set contamination from the subsequent public release of HumanEval. Similar to GPT-3 and unlike Codex, we train our models from scratch from a random initialization.",No,Yes,
12/29/2023 15:49:28,Pangu-sigma,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,"@article{ren2023pangu,   title={PanGu-$\{$$\backslash$Sigma$\}$: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},   author={Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and others},   journal={arXiv preprint arXiv:2303.10845},   year={2023} }",\cite{ren2023pangu},2023,Yes,Yes,No,Yes,"GH Torrent, the Pile, Pangu-Coder","Java, Python","For code, we use the Python code (147GB) which has been used in PanGu-Coder [33], as well as the Java code (161GB) from GHTorrent [34] , which are then filtered by file size (<1MB), average number of characters per line (<200), maximum number of characters per line (<1000) and their compilablity. Then, these collected English, Chinese and code texts data was sampled and distributed to the four major domains. Finally, we get more than 300B tokens for the four major domains. The detailed statistics of data distribution and data sources in four major domains are presented in Table 1.
For the remaining 36 domains, the data for 26 monolingual domains are mainly from CCAligned [35 ] and CCMatrix [ 36]. Similar to the code domain mentioned above, the data for 6 programming language domains are collected through GHTorrent [ 34 ] and filtered in the similar way.",No,Yes,
12/29/2023 15:56:37,PaLM2,PaLM 2 Technical Report,"@article{anil2023palm,   title={Palm 2 technical report},   author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},   journal={arXiv preprint arXiv:2305.10403},   year={2023} }",\cite{anil2023palm},2023,Yes,Yes,No,Yes,,,"The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM",No,Yes,
12/29/2023 16:01:05,LLama 2,Llama 2: Open Foundation and Fine-Tuned Chat Models,"@article{touvron2023llama,   title={Llama 2: Open foundation and fine-tuned chat models},   author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},   journal={arXiv preprint arXiv:2307.09288},   year={2023} }",\cite{touvron2023llama},2023,Yes,Yes,Yes,No,BigQuery from LLaMa 1,,"Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.",Yes,Yes,Doesnt say it uses code explicitly but can be assumed from the evaluation
12/29/2023 16:17:04,Falcon,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only","@article{penedo2023refinedweb,   title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},   author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},   journal={arXiv preprint arXiv:2306.01116},   year={2023} }",\cite{penedo2023refinedweb},2023,Yes,Yes,No,No,The Pile,,We train internal models on both The Pile and RefinedWeb to control for deviations caused by our pretraining setup–we found The Pile models to perform in-line with others.,Yes,Yes,
12/29/2023 16:27:21,Mistral,Mistral 7B,"@article{jiang2023mistral,   title={Mistral 7B},   author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},   journal={arXiv preprint arXiv:2310.06825},   year={2023} }",\cite{jiang2023mistral},2023,Yes,Yes,No,,,,,No,Yes,
12/29/2023 16:30:18,BERT,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"@inproceedings{Devlin2019BERTPO,   title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},   author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},   booktitle={North American Chapter of the Association for Computational Linguistics},   year={2019},   url={https://api.semanticscholar.org/CorpusID:52967399} }",\cite{Devlin2019BERTPO},2019,Yes,No,,,,,,,,
12/29/2023 16:46:07,WizardLM,WizardLM: Empowering Large Language Models to Follow Complex Instructions,"@article{xu2023wizardlm,   title={Wizardlm: Empowering large language models to follow complex instructions},   author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},   journal={arXiv preprint arXiv:2304.12244},   year={2023} }",\cite{xu2023wizardlm},2023,Yes,Yes,Yes,No,BigQuery from LLaMa,,,Yes,Yes,
12/29/2023 16:51:42,mBART,Multilingual Denoising Pre-training for Neural Machine Translation,"@article{liu2020multilingual,   title={Multilingual Denoising Pre-training for Neural Machine Translation},   author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},   journal={Transactions of the Association for Computational Linguistics},   volume={8},   pages={726--742},   year={2020} }",\cite{liu2020multilingual},2020,Yes,No,,,,,,,,
12/29/2023 16:54:16,CodeBERT,CodeBERT: A Pre-Trained Model for Programming and Natural Languages,"@inproceedings{feng2020codebert,   title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},   author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},   booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},   pages={1536--1547},   year={2020} }",\cite{feng2020codebert},2020,Yes,Yes,,,,,,,No,
12/29/2023 16:57:31,GPT-1,Improving Language Understanding by Generative Pre-Training,"@article{radford2018improving,   title={Improving language understanding by generative pre-training},   author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others} }",\cite{radford2018improving},2018,Yes,No,,,,,,,,
12/29/2023 17:01:45,XLNet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"@article{yang2019xlnet,   title={Xlnet: Generalized autoregressive pretraining for language understanding},   author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},   journal={Advances in neural information processing systems},   volume={32},   year={2019} }",\cite{yang2019xlnet},2019,Yes,No,,,,,,,,
12/29/2023 17:20:27,GPT-J,GPT-J-6B: A 6 billion parameter autoregressive language model.,"@misc{gpt-j,   author = {Wang, Ben and Komatsuzaki, Aran},   title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},   howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},   year = 2021,   month = May }",\cite{gpt-j},2021,Yes,Yes,No,No,The Pile,"Assembly', 'Batchfile', 'C', 'C#', 'C++', 'CMake', 'COBOL', 'CSS', 'CSV', 'Clojure', 'CoffeeScript', 'DM', 'Dart', 'Dockerfile', 'Elixir', 'Erlang', 'Fortran', 'Go', 'Groovy', 'HTML', 'Haskell', 'INI', 'JSON', 'Java', 'JavaScript', 'Julia', 'Kotlin', 'Lisp', 'Lua', 'Makefile', 'Markdown', 'Matlab', 'None', 'OCaml', 'Objective-C', 'PHP', 'Pascal', 'Perl', 'PowerShell', 'Prolog', 'Python', 'R', 'Ruby', 'Rust', 'SQL', 'Scala', 'Shell', 'Swift', 'TOML', 'TeX', 'TypeScript', 'Verilog', 'Visual Basic', 'XML', 'YAML'",The model was trained on 400B tokens from [The Pile](https://pile.eleuther.ai/) dataset with 800GB text.,Yes,Yes,
12/29/2023 17:35:01,GPT-C,IntelliCode Compose: Code Generation using Transformer,"@misc{svyatkovskiy2020intellicode,       title={IntelliCode Compose: Code Generation Using Transformer},        author={Alexey Svyatkovskiy and Shao Kun Deng and Shengyu Fu and Neel Sundaresan},       year={2020},       eprint={2005.08025},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{svyatkovskiy2020intellicode},2020,Yes,Yes,No,Yes,,"Python, C#, JavaScript,TypeScript","We collect a large unsupervised source code dataset to train and
evaluate the code sequence completion model. It comprises over 1.2
billion lines of source code in Python, C#, JavaScript and TypeScript
programming languages, as summarized in Tab. 1. A total of over
52000 top-starred (non-fork) projects in GitHub has been selected,
containing libraries from a diverse set of domains, with over 4.7
million source code files. We split the dataset into development and test sets in the proportion 70-30 on the repository level. The development set is then split
at random into training and validation sets in the proportion 80-20.
The final deployed model is retrained using the entire dataset.",No,Yes,
12/29/2023 18:08:11,CodeRetriever,CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search,"@misc{li2022coderetriever,       title={CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search},        author={Xiaonan Li and Yeyun Gong and Yelong Shen and Xipeng Qiu and Hang Zhang and Bolun Yao and Weizhen Qi and Daxin Jiang and Weizhu Chen and Nan Duan},       year={2022},       eprint={2201.10866},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{li2022coderetriever},2022,Yes,Yes,,,,,,,No,Trained purely on CodeSearchNet which is not file level
12/29/2023 18:13:02,PRCBERT,PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models,"@inproceedings{prcbert, author = {Luo, Xianchang and Xue, Yinxing and Xing, Zhenchang and Sun, Jiamou}, title = {PRCBERT: Prompt Learning for Requirement Classification Using BERT-Based Pretrained Language Models}, year = {2023}, isbn = {9781450394758}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3551349.3560417}, doi = {10.1145/3551349.3560417}, abstract = {Software requirement classification is a longstanding and important problem in requirement engineering. Previous studies have applied various machine learning techniques for this problem, including Support Vector Machine (SVM) and decision trees. With the recent popularity of NLP technique, the state-of-the-art approach NoRBERT utilizes the pre-trained language model BERT and achieves a satisfactory performance. However, the dataset PROMISE used by the existing approaches for this problem consists of only hundreds of requirements that are outdated according to today’s technology and market trends. Besides, the NLP technique applied in these approaches might be obsolete. In this paper, we propose an approach of prompt learning for requirement classification using BERT-based pretrained language models (PRCBERT), which applies flexible prompt templates to achieve accurate requirements classification. Experiments conducted on two existing small-size requirement datasets (PROMISE and NFR-Review) and our collected large-scale requirement dataset NFR-SO prove that PRCBERT exhibits moderately better classification performance than NoRBERT and MLM-BERT (BERT with the standard prompt template). On the de-labeled NFR-Review and NFR-SO datasets, Trans_PRCBERT (the version of PRCBERT which is fine-tuned on PROMISE) is able to have a satisfactory zero-shot performance with 53.27\% and 72.96\% F1-score when enabling a self-learning strategy.}, booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering}, articleno = {75}, numpages = {13}, keywords = {requirement auto-labeling, prompt learning, self-learning strategy, requirement classification, zero-shot learning}, location = {, Rochester, MI, USA, }, series = {ASE '22} }",\cite{prcbert},2023,Yes,No,,,,,,,,"Based on BERT (encoder only) and is only used for classification in the paper, so I marked it as non generative."
12/29/2023 18:21:39,seBERT,Based on BERT and is only used for classification in the paper.,"@article{Trautsch2022PredictingIT,   title={Predicting Issue Types with seBERT},   author={Alexander Trautsch and Steffen Herbold},   journal={2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE)},   year={2022},   pages={37-39},   url={https://api.semanticscholar.org/CorpusID:248506056} }",\cite{Trautsch2022PredictingIT},2022,Yes,No,,,,,,,,"Also based on BERT (encoder only) but it is used for issue prediction in the paper, so I marked it as generative."
12/29/2023 19:18:28,TraceBERT,Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models,"@misc{lin2021traceability,       title={Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},        author={Jinfeng Lin and Yalin Liu and Qingkai Zeng and Meng Jiang and Jane Cleland-Huang},       year={2021},       eprint={2102.04411},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{lin2021traceability},2021,Yes,Yes,,,,,,,No,
12/29/2023 19:23:51,GraphCodeBERT,GraphCodeBERT: Pre-training Code Representations with Data Flow,"@misc{guo2021graphcodebert,       title={GraphCodeBERT: Pre-training Code Representations with Data Flow},        author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},       year={2021},       eprint={2009.08366},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{guo2021graphcodebert},2021,Yes,Yes,,,,,,,No,
12/29/2023 21:17:13,BERT Overflow,Code and Named Entity Recognition in StackOverflow,"@inproceedings{tabassum-etal-2020-code,     title = ""Code and Named Entity Recognition in {S}tack{O}verflow"",     author = ""Tabassum, Jeniya  and       Maddela, Mounica  and       Xu, Wei  and       Ritter, Alan"",     editor = ""Jurafsky, Dan  and       Chai, Joyce  and       Schluter, Natalie  and       Tetreault, Joel"",     booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",     month = jul,     year = ""2020"",     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2020.acl-main.443"",     doi = ""10.18653/v1/2020.acl-main.443"",     pages = ""4913--4926"",}",\cite{tabassum-etal-2020-code},2020,Yes,Yes,,,,,,,No,
12/29/2023 21:36:29,ALBERT ,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"@misc{lan2020albert,       title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},        author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},       year={2020},       eprint={1909.11942},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{lan2020albert},2020,Yes,No,,,,,,,,
12/29/2023 21:45:05,RoBERTa,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"@misc{liu2019roberta,       title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},        author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},       year={2019},       eprint={1907.11692},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{liu2019roberta},2019,Yes,No,,,,,,,,
12/29/2023 21:53:57,CodeT5+,CodeT5+: Open Code Large Language Models for Code Understanding and Generation,"@misc{wang2023codet5,       title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},        author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},       year={2023},       eprint={2305.07922},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{wang2023codet5},2023,Yes,Yes,Yes,No,"CodeSearchNet, GitHub Code Dataset (latter one is file level)","Python, Java, Ruby, JavaScript, Go, PHP, C, C++, C#","We enlarge the pretraining dataset of CodeSearchNet [Husain et al., 2019] with the recently released GitHub Code dataset3. We select nine PLs (Python, Java, Ruby, JavaScript, Go, PHP, C, C++, C#) and filter the dataset by preserving only permissively licensed code4 and files with 50 to 2000 tokens. Besides, we filter out the overlapped subset with CodeSearchNet and other downstream tasks covered in our evaluation by checking their GitHub repository names. Note that although we employ the deduplicated data version in which duplicates are filtered out based on the exact match (ignoring whitespaces), there might be some potential remaining duplicates. However, we do not expect any remaining duplication will impact our model performance significantly. We use the CodeT5 tokenizer to tokenize the multilingual dataset, resulting in 51.5B tokens, ∼50x larger than CodeSearchNet.",Yes,Yes,
12/29/2023 22:06:11,UnixCoder ,UniXcoder: Unified Cross-Modal Pre-training for Code Representation,"@misc{guo2022unixcoder,       title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},        author={Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},       year={2022},       eprint={2203.03850},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{guo2022unixcoder},2022,Yes,Yes,,,,,,,No,
12/30/2023 10:22:54,CoText,CoTexT: Multi-task Learning with Code-Text Transformer,"@misc{phan2021cotext,       title={CoTexT: Multi-task Learning with Code-Text Transformer},        author={Long Phan and Hieu Tran and Daniel Le and Hieu Nguyen and James Anibal and Alec Peltekian and Yanfang Ye},       year={2021},       eprint={2105.08645},       archivePrefix={arXiv},       primaryClass={cs.AI} }",\cite{phan2021cotext},2021,Yes,Yes,,,,,,,No,"It says it was trained on both CodeSearchNet and GitHub Code Dataset, but they extract the functions from the github file, so it is not file level."
12/30/2023 13:35:52,WizardCoder,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,"@misc{luo2023wizardcoder,       title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},        author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},       year={2023},       eprint={2306.08568},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{luo2023wizardcoder},2023,Yes,Yes,Yes,No,"The Stack (used through StarCoder), Code Alpaca for fine tuning with Evol Instruct",All languages in The Stack,"We employ the following procedure to train WizardCoder. Initially, we utilize StarCoder 15B as the foundation and proceed to fine-tune it using the code instruction-following training set, which was evolved through Evol-Instruct. To construct the training dataset, we initialized it with the 20K instruction-following dataset called Code Alpaca .We iteratively employ the Evol-Instruct technique on this dataset consisting of 20,000 samples to produce evolved data. After each round of data evolution, we merge the evolved data from all previous rounds with the original dataset to finetune StarCoder and assess the pass@1 metric on HumanEval. Once we observe a decline in the pass@1 metric, we will discontinue the usage of Evol-Instruct and choose the model with the highest pass@1 as the ultimate model.",Yes,Yes,
12/30/2023 14:34:57,GPT-Neo,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,"@software{black_2021_5297715,   author       = {Black, Sid and                   Leo, Gao and                   Wang, Phil and                   Leahy, Connor and                   Biderman, Stella},   title        = {{GPT-Neo: Large Scale Autoregressive Language                     Modeling with Mesh-Tensorflow}},   month        = aug,   year         = 2021,   publisher    = {Zenodo},   version      = {1.0},   doi          = {10.5281/zenodo.5297715},   url          = {https://doi.org/10.5281/zenodo.5297715} }",\cite{black_2021_5297715},2021,Yes,Yes,No,No,ThePile,All languages in ThePile,"GPT-Neo 2.7B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.",Yes,Yes,
12/30/2023 15:02:35,CodeParrot,Natural Language Processing with Transformers,"@book{tunstall2022nlp,   author    = {Lewis Tunstall and Leandro von Werra and Thomas Wolf},   title     = {Natural Language Processing with Transformers},   year      = {2022},   publisher = {O'Reilly Media, Inc.} }",\cite{tunstall2022nlp},2022,Yes,Yes,No,Yes,CodeParrot Dataset,Python,The model was trained on the cleaned CodeParrot 🦜 dataset in two steps. After the initial training (v1.0) the model was trained for another 30k steps resulting in v1.1.,Yes,Yes,
12/30/2023 16:27:18,SantaCoder,SantaCoder: don't reach for the stars!,"@misc{allal2023santacoder,       title={SantaCoder: don't reach for the stars!},        author={Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Munoz Ferrandis and Niklas Muennighoff and Mayank Mishra and Alex Gu and Manan Dey and Logesh Kumar Umapathi and Carolyn Jane Anderson and Yangtian Zi and Joel Lamy Poirier and Hailey Schoelkopf and Sergey Troshin and Dmitry Abulkhanov and Manuel Romero and Michael Lappert and Francesco De Toni and Bernardo García del Río and Qian Liu and Shamik Bose and Urvashi Bhattacharyya and Terry Yue Zhuo and Ian Yu and Paulo Villegas and Marco Zocca and Sourab Mangrulkar and David Lansky and Huu Nguyen and Danish Contractor and Luis Villa and Jia Li and Dzmitry Bahdanau and Yacine Jernite and Sean Hughes and Daniel Fried and Arjun Guha and Harm de Vries and Leandro von Werra},       year={2023},       eprint={2301.03988},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{allal2023santacoder},2023,Yes,Yes,Yes,No,The Stack v1.1,"Python, Java, JavaScript","The base training dataset for the experiments in this paper contains 268 GB of Python,
Java and JavaScript files from The Stack v1.1 (Kocetkov et al., 2022) after removing data from optout requests, near-deduplication, PII-redaction (see Section 4), and filtering based on line-length and percentage of alphanumeric characters. This dataset was also decontaminated by removing files that contained test-samples from the following benchmarks: HumanEval (Chen et al., 2021), APPS (Hendrycks et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2022).",Yes,Yes,
12/30/2023 16:54:15,PanGu-Coder,PanGu-Coder: Program Synthesis with Function-Level Language Modeling,"@misc{christopoulou2022pangucoder,       title={PanGu-Coder: Program Synthesis with Function-Level Language Modeling},        author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhongqi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li Yan and Pingyi Zhou and Xin Wang and Yuchi Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jiansheng Wei and Xin Jiang and Qianxiang Wang and Qun Liu},       year={2022},       eprint={2207.11280},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{christopoulou2022pangucoder},2022,Yes,Yes,,,,,,,No,"Custom Python GitHub scrape, but only extract functions, so it is not file level."
12/30/2023 16:56:59,PanGu-Coder-FT,PanGu-Coder: Program Synthesis with Function-Level Language Modeling,"@misc{christopoulou2022pangucoder,       title={PanGu-Coder: Program Synthesis with Function-Level Language Modeling},        author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhongqi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li Yan and Pingyi Zhou and Xin Wang and Yuchi Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jiansheng Wei and Xin Jiang and Qianxiang Wang and Qun Liu},       year={2022},       eprint={2207.11280},       archivePrefix={arXiv},       primaryClass={cs.LG} }",\cite{christopoulou2022pangucoder},2022,Yes,Yes,,,,,,,No,
12/30/2023 17:20:09,PolyCoder,A Systematic Evaluation of Large Language Models of Code,"@misc{xu2022systematic,       title={A Systematic Evaluation of Large Language Models of Code},        author={Frank F. Xu and Uri Alon and Graham Neubig and Vincent J. Hellendoorn},       year={2022},       eprint={2202.13169},       archivePrefix={arXiv},       primaryClass={cs.PL} }",\cite{xu2022systematic},2022,Yes,Yes,No,Yes,,"C, C++, C#, Go, Java, JavaScript, PHP, Python, Ruby, Rust, Scala, TypeScript ","To fill this gap, we train a 2.7 billion model, PolyCoder, on a mixture of
repositories from GitHub in 12 different programming languages.
GitHub is an excellent source for publicly available source code of various programming languages. We cloned the most popular repositories for 12 popular programming languages with at least 50 stars (stopping at about 25K per language to avoid a too heavy skew towards popular programming languages) from GitHub in October 2021. For each project, each file belonging to the majority-language of that project was extracted, yielding the initial training set. This initial, unfiltered dataset spanned 631GB and 38.9M files.",Yes,Yes,
1/2/2024 19:22:58,"GPT-3.5 (code-davinci-002, text-davinci-002, text-davinci-003, gpt-3.5-turbo)",,,,2022,Yes,Yes,No,,,Contains at least all datasets used for training Codex.,,No,Yes,
1/2/2024 19:46:58,PaLM-Coder,PaLM: Scaling Language Modeling with Pathways,"@misc{chowdhery2022palm,       title={PaLM: Scaling Language Modeling with Pathways},        author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},       year={2022},       eprint={2204.02311},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{chowdhery2022palm},2022,Yes,Yes,Yes,Yes,"PaLM for pre-training, finetuned on ExtraPythonData ","Java, HTML, JavaScript, Python, C, PHP, C#, C++, XML, Ruby, Go, Matlab, TypeScript, Shell, Rust, SQL, Lua, Notebooks, Dart, Other, Visual Basic, Perl, S, Clojure, Emacs Lisp, TCL","In addition to natural language data, the pretraining dataset also contains code. The source code in the pretraining dataset is obtained from open source repositories on GitHub. We filtered the files by the license included in the repository; copyleft licenses were excluded. We filter the files by filename extension to restrict to one of 24 common programming languages, including Java, HTML, Javascript, Python, PHP, C#, XML, C++, and C, which results in 196GB of source code. Further, we remove duplicates based on Levenshtein
distance between the files because duplicate files are known to be common in source code repositories (Lopes et al., 2017; Allamanis, 2019).
PaLM-Coder. Now we evaluate the effect of further finetuning only on code, akin to performed by Chen et al. (2021). We call the resulting models PaLM-Coder in Table 12. We finetune the 8B, 62B, and 540B PaLM models in two stages: (a) first finetuning for 6.5B tokens on a mixture of 60% Python code from ExtraPythonData, 30% code across languages (from the same source as the pre-training code data, but which was not included in pretraining), and 10% natural language, and (b) finetuning for an additional 1.9B tokens on more Python code from ExtraPythonData. ",No,Yes,
1/2/2024 20:13:02,CodeGPT,CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"@misc{lu2021codexglue,       title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},        author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},       year={2021},       eprint={2102.04664},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{lu2021codexglue},2021,Yes,Yes,,,,,,,No,
1/2/2024 20:14:13,CodeGPT-adapted,CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"@misc{lu2021codexglue,       title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},        author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},       year={2021},       eprint={2102.04664},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{lu2021codexglue},2021,Yes,Yes,,,,,,,No,
1/2/2024 20:40:43,PanGu-Coder2,PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback,"@misc{shen2023pangucoder2,       title={PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback},        author={Bo Shen and Jiaxin Zhang and Taihong Chen and Daoguang Zan and Bing Geng and An Fu and Muhan Zeng and Ailun Yu and Jichuan Ji and Jingyang Zhao and Yuenan Guo and Qianxiang Wang},       year={2023},       eprint={2307.14936},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{shen2023pangucoder2},2023,Yes,Yes,,,,,,,No,
1/2/2024 21:02:30,Code Llama,Code Llama: Open Foundation Models for Code,"@misc{rozière2023code,       title={Code Llama: Open Foundation Models for Code},        author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},       year={2023},       eprint={2308.12950},       archivePrefix={arXiv},       primaryClass={cs.CL} }",\cite{rozière2023code},2023,Yes,Yes,No,Yes,Code Llama dataset  + Llama 2 dataset,"At least Python, C++, Java, PHP, TypeScript, C#, and Bash","We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2. As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of publicly available code. We also source 8% of our samples data from natural language datasets related tocode. This dataset contains many discussions about code and code snippets included in natural language questions or answers. To help the model retain natural language understanding skills, we also sample a small proportion of our batches from a natural language dataset. Data is tokenized via byte pair encoding (BPE, Sennrich et al. (2016)), employing the same tokenizer as Llama and Llama 2. Preliminary experiments suggested that adding batches sampled from our natural language dataset improves the performance of our models on MBPP.",No,Yes,"Not 100% sure if file level, little info about dataset"
1/2/2024 21:18:51,CoditT5,CoditT5: Pretraining for Source Code and Natural Language Editing,"@misc{zhang2022coditt5,       title={CoditT5: Pretraining for Source Code and Natural Language Editing},        author={Jiyang Zhang and Sheena Panthaplackel and Pengyu Nie and Junyi Jessy Li and Milos Gligoric},       year={2022},       eprint={2208.05446},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{zhang2022coditt5},2022,Yes,Yes,,,,,,,No,"Uses CodeSearchNet and other datasets for finetuning, however, none of them uses file level code."
1/2/2024 23:38:58,SPT-Code,SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations,"@misc{niu2022sptcode,       title={SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations},        author={Changan Niu and Chuanyi Li and Vincent Ng and Jidong Ge and Liguo Huang and Bin Luo},       year={2022},       eprint={2201.01549},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{niu2022sptcode},2022,Yes,Yes,,,,,,,No,
1/2/2024 23:48:43,CuBERT,Learning and Evaluating Contextual Embedding of Source Code,"@misc{kanade2020learning,       title={Learning and Evaluating Contextual Embedding of Source Code},        author={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},       year={2020},       eprint={2001.00059},       archivePrefix={arXiv},       primaryClass={cs.SE} }",\cite{kanade2020learning},2020,Yes,Yes,No,No,"BigQuery, ETH Py150 Open corpus",Python,"We use the ETH Py150 corpus (Raychev et al., 2016) to generate datasets for the fine-tuning tasks. This corpus consists of 150K Python files from GitHub, and is partitioned into a training split (100K files) and a test split (50K files). We held out 10K files from the training split as a validation split. We deduplicated the dataset in the fashion of Allamanis
(2018). Finally, we drop from this corpus those projects for which licensing information was not available or whose licenses restrict use or redistribution. We call the resulting
corpus the ETH Py150 Open corpus.2 This is our Python fine-tuning code corpus, and it consists of 74,749 training files, 8,302 validation files, and 41,457 test files. 
We used the public GitHub repository hosted on Google’s BigQuery platform (the github repos dataset under BigQuery’s public-data project, bigquery-public-data).
We extracted all files ending in .py, under open-source, redistributable licenses, removed symbolic links, and retained only files reported to be in the refs/heads/master
branch. This resulted in about 16.2 million files.",Yes,Yes,
1/2/2024 23:59:35,T5-Learning,Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,"@article{Mastropaolo2021StudyingTU,   title={Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},   author={Antonio Mastropaolo and Simone Scalabrino and Nathan Cooper and David Nader-Palacio and Denys Poshyvanyk and Rocco Oliveto and Gabriele Bavota},   journal={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   year={2021},   pages={336-347},   url={https://api.semanticscholar.org/CorpusID:231786586} }",\cite{Mastropaolo2021StudyingTU},2021,Yes,Yes,,,,,,,No,
1/3/2024 10:46:16,Vicuna,Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality,"@misc{vicuna2023,     title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},     url = {https://lmsys.org/blog/2023-03-30-vicuna/},     author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},     month = {March},     year = {2023} }",\cite{vicuna2023},2023,Yes,Yes,Yes,No,"BigQuery, user-shared conversations collected from ShareGPT","Vicuna is a Llama model fine-tuned on user-shared conversations. So, it contains the same training dataset as Llama, namely BigQuery (all languages).",Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. ,Yes,Yes,
1/3/2024 11:15:55,Stable Code,,"@misc{StableCodeCompleteAlpha4K,        url={[https://huggingface.co/stabilityai/stablecode-complete-alpha-3b-4k](https://huggingface.co/stabilityai/stablecode-complete-alpha-3b-4k)},        title={Stable Code Complete Alpha},        author={Adithyan, Reshinth and Phung, Duy and Cooper, Nathan and Pinnaparaju, Nikhil and Laforte, Christian} }",\cite{StableCodeCompleteAlpha4K},2023,Yes,Yes,Yes,No,The Stack v1.2,Languages in The Stack,"StableCode offers a unique way for developers to become more efficient by using three different models to help in their coding. The base model was first trained on a diverse set of programming languages from the stack-dataset (v1.2) from BigCode and then trained further with popular languages like Python, Go, Java, Javascript, C, markdown and C++.  In total, we trained our models on 560B tokens of code on our HPC cluster. ",Yes,Yes,
1/3/2024 11:22:30,StableLM,StableLM Alpha v2 Models,"@misc{StableLMAlphaV2Models, 
      url={[https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2](https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2)},
      title={StableLM Alpha v2 Models},
      author={Tow, Jonathan}
}",\cite{StableLMAlphaV2Models},2023,Yes,Yes,Yes,No,"Falcon RefinedWeb extract, and RedPajama-Data, and The Pile both without Books3 and other subsets, and StarCoder.",All languages in ThePile and RedPajama,"The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), and RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without Books3 and other subsets, and StarCoder (Li et al., 2023).",Yes,Yes,
1/3/2024 11:32:55,Stable LM Zephyr,,,,2023,Yes,Yes,Yes,No,Same datasets used for StableLM,Same as StableLM,"The development of Stable LM Zephyr 3B focused on creating a model that performs well in text generation and aligns human preferences. Inspired by Zephyr 7B, we adapted its training pipeline, with the first step being supervised fine-tuning on multiple instruction datasets, including UltraChat, MetaMathQA, Evol Wizard Dataset, & Capybara Dataset. In the second step, we aligned the model with the Direct Preference Optimization (DPO) algorithm utilizing the UltraFeedback dataset. This dataset is from the OpenBMB research group and comprises 64,000 prompts and corresponding model responses. Recently released models, such as Zephyr-7B, Neural-Chat-7B, and Tulu-2-DPO-70B, successfully used Direct Preference Optimization (DPO). However, Stable Zephyr is one of the first models of this type yet with the efficient size of 3B parameters.",Yes,Yes,
1/3/2024 11:36:54,Stable Beluga,Stable Beluga models,"@misc{StableBelugaModels, 
      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, 
      title={Stable Beluga models}, 
      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}
}",\cite{StableBelugaModels},2023,Yes,Yes,Yes,No,"Since it is based on LLaMa, it has the same training dataset, namely BigQuery + Orca style dataset. ",Same as LLaMa.,"The training for the Stable Beluga models was directly inspired by the methodology pioneered by Microsoft in its paper: ""Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources.

Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used), was created synthetically using high-quality instructions from the following datasets created by Enrico Shippole:

COT Submix Original

NIV2 Submix Original

FLAN 2021 Submix Original

T0 Submix Original",Yes,Yes,"Assumption about BigQuery, since it is based on LLaMa."
1/3/2024 11:54:06,Japanese StableLM,Japanese StableLM Base Alpha 7B,"@misc{JapaneseStableLMBaseAlpha7B,        url={[https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)},        title={Japanese StableLM Base Alpha 7B},        author={Lee, Meng and Nakamura, Fujiki and Shing, Makoto and McCann, Paul and Akiba, Takuya and Orii, Naoki} }",\cite{JapaneseStableLMBaseAlpha7B},2023,Yes,Yes,Yes,No,"Japanese/English Wikipedia, Japanese mc4, Japanese CC-100, Japanese OSCAR, RedPajama",All languages in RedPjama (unspecified) ,"Japanese StableLM Base Alpha 7B is trained for text generation using large-scale data sourced mainly from the Web. The training data is predominantly composed of Japanese and English text, with the remaining 2 percent of material in the form of source code. 
In addition to open datasets, the training data includes datasets created by Stability AI Japan and datasets created with the cooperation of the Japanese team of the EleutherAI Polyglot project, along with members of Stability AI Japan's community.",Yes,Yes,