

| Model Name | Source |
|--|--|
| CodeT5 | @inproceedings{wang2021codet5,     title = "{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",     author = "Wang, Yue  and       Wang, Weishi  and       Joty, Shafiq  and       Hoi, Steven C.H.",     editor = "Moens, Marie-Francine  and       Huang, Xuanjing  and       Specia, Lucia  and       Yih, Scott Wen-tau",     booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",     month = nov,     year = "2021",     address = "Online and Punta Cana, Dominican Republic",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2021.emnlp-main.685",     doi = "10.18653/v1/2021.emnlp-main.685",     pages = "8696--8708",     abstract = "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at \url{https://github.com/salesforce/CodeT5}.", } |
| CodeGen | @inproceedings{ nijkamp2023codegen, title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis}, author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=iaYcJKpY2B_} } |
| GPT-3 | @article{brown2020language,   title={Language models are few-shot learners},   author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},   journal={Advances in neural information processing systems},   volume={33},   pages={1877--1901},   year={2020} } |
| InCoder | @inproceedings{ fried2023incoder, title={InCoder: A Generative Model for Code Infilling and Synthesis}, author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Scott Yih and Luke Zettlemoyer and Mike Lewis}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=hQwb-lbM6EL} } |
| T5 | @article{raffel2020exploring,   title={Exploring the limits of transfer learning with a unified text-to-text transformer},   author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},   journal={The Journal of Machine Learning Research},   volume={21},   number={1},   pages={5485--5551},   year={2020},   publisher={JMLRORG} } |
| GPT-2 | @inproceedings{Radford2019LanguageMA,   title={Language Models are Unsupervised Multitask Learners},   author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},   year={2019},   url={https://api.semanticscholar.org/CorpusID:160025533} } |
| PLBART | @inproceedings{ahmad2021unified,   title={Unified Pre-training for Program Understanding and Generation},   author={Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},   booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},   pages={2655--2668},   year={2021} } |
| BART | @inproceedings{lewis-etal-2020-bart,     title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",     author = "Lewis, Mike  and       Liu, Yinhan  and       Goyal, Naman  and       Ghazvininejad, Marjan  and       Mohamed, Abdelrahman  and       Levy, Omer  and       Stoyanov, Veselin  and       Zettlemoyer, Luke",     editor = "Jurafsky, Dan  and       Chai, Joyce  and       Schluter, Natalie  and       Tetreault, Joel",     booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",     month = jul,     year = "2020",     address = "Online",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2020.acl-main.703",     doi = "10.18653/v1/2020.acl-main.703",     pages = "7871--7880",     abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", } |     
| mT5 | @inproceedings{xue2021mt5,   title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},   author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},   booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},   pages={483--498},   year={2021} } | 
| CPM-2 | @misc{zhang2021cpm2,       title={CPM-2: Large-scale Cost-effective Pre-trained Language Models},        author={Zhengyan Zhang and Yuxian Gu and Xu Han and Shengqi Chen and Chaojun Xiao and Zhenbo Sun and Yuan Yao and Fanchao Qi and Jian Guan and Pei Ke and Yanzheng Cai and Guoyang Zeng and Zhixing Tan and Zhiyuan Liu and Minlie Huang and Wentao Han and Yang Liu and Xiaoyan Zhu and Maosong Sun},       year={2021},       eprint={2106.10715},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| PanGu-α | @article{weizeng,   author       = {Wei Zeng and                   Xiaozhe Ren and                   Teng Su and                   Hui Wang and                   Yi Liao and                   Zhiwei Wang and                   Xin Jiang and                   ZhenZhang Yang and                   Kaisheng Wang and                   Xiaoda Zhang and                   Chen Li and                   Ziyan Gong and                   Yifan Yao and                   Xinjing Huang and                   Jun Wang and                   Jianfeng Yu and                   Qi Guo and                   Yue Yu and                   Yan Zhang and                   Jin Wang and                   Hengtao Tao and                   Dasen Yan and                   Zexuan Yi and                   Fang Peng and                   Fangqing Jiang and                   Han Zhang and                   Lingfeng Deng and             Yehong Zhang and                   Zhe Lin and                   Chao Zhang and                   Shaojie Zhang and                   Mingyue Guo and                   Shanzhi Gu and                   Gaojun Fan and                   Yaowei Wang and                   Xuefeng Jin and                   Qun Liu and                   Yonghong Tian},   title        = {PanGu-{\(\alpha\)}: Large-scale Autoregressive Pretrained Chinese                   Language Models with Auto-parallel Computation},   journal      = {CoRR},   volume       = {abs/2104.12369},   year         = {2021},   url          = {https://arxiv.org/abs/2104.12369},   eprinttype    = {arXiv},   eprint       = {2104.12369},   timestamp    = {Tue, 31 Oct 2023 15:43:37 +0100},   biburl       = {https://dblp.org/rec/journals/corr/abs-2104-12369.bib},   bibsource    = {dblp computer science bibliography, https://dblp.org} } |
| T0 | @misc{sanh2022multitask,       title={Multitask Prompted Training Enables Zero-Shot Task Generalization},        author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},       year={2022},       eprint={2110.08207},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| GPT-NeoX | @inproceedings{black-etal-2022-gpt,     title = "{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",     author = "Black, Sidney  and       Biderman, Stella  and       Hallahan, Eric  and       Anthony, Quentin  and       Gao, Leo  and       Golding, Laurence  and       He, Horace  and       Leahy, Connor  and       McDonell, Kyle  and       Phang, Jason  and       Pieler, Michael  and       Prashanth, Usvsn Sai  and       Purohit, Shivanshu  and       Reynolds, Laria  and       Tow, Jonathan  and       Wang, Ben  and       Weinbach, Samuel",     editor = "Fan, Angela  and       Ilic, Suzana  and       Wolf, Thomas  and       Gall{\'e}, Matthias",     booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",     month = may,     year = "2022",     address = "virtual+Dublin",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.bigscience-1.9",     doi = "10.18653/v1/2022.bigscience-1.9",     pages = "95--136", } |
| Tk-Instruct | @inproceedings{wang-etal-2022-super,     title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",     author = "Wang, Yizhong  and       Mishra, Swaroop  and       Alipoormolabashi, Pegah  and       Kordi, Yeganeh  and       Mirzaei, Amirreza  and       Naik, Atharva  and       Ashok, Arjun  and       Dhanasekaran, Arut Selvan  and       Arunkumar, Anjana  and       Stap, David  and       Pathak, Eshaan  and       Karamanolakis, Giannis  and       Lai, Haizhi  and       Purohit, Ishan  and       Mondal, Ishani  and       Anderson, Jacob  and       Kuznia, Kirby  and       Doshi, Krima  and       Pal, Kuntal Kumar  and       Patel, Maitreya  and       Moradshahi, Mehrad  and       Parmar, Mihir  and       Purohit, Mirali  and       Varshney, Neeraj  and       Kaza, Phani Rohitha  and       Verma, Pulkit  and       Puri, Ravsehaj Singh  and       Karia, Rushang  and       Doshi, Savan  and       Sampat, Shailaja Keyur  and       Mishra, Siddhartha  and       Reddy A, Sujan  and       Patro, Sumanta  and       Dixit, Tanay  and       Shen, Xudong",     editor = "Goldberg, Yoav  and       Kozareva, Zornitsa  and       Zhang, Yue",     booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",     month = dec,     year = "2022",     address = "Abu Dhabi, United Arab Emirates",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2022.emnlp-main.340",     doi = "10.18653/v1/2022.emnlp-main.340",     pages = "5085--5109",} |
| UL2 | @article{tay2022unifying,   title={Unifying language learning paradigms},   author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},   journal={arXiv preprint arXiv:2205.05131},   year={2022} } |
| OPT | @article{zhang2022opt,   title={Opt: Open pre-trained transformer language models},   author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},   journal={arXiv preprint arXiv:2205.01068},   year={2022} } |
| NLLB | @article{costa2022no,   title={No language left behind: Scaling human-centered machine translation},   author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},   journal={arXiv preprint arXiv:2207.04672},   year={2022} } |
| CodeGeeX | @article{zheng2023codegeex,   title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},   author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},   journal={arXiv preprint arXiv:2303.17568},   year={2023} } |
| GLM | @inproceedings{du2022glm,   title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},   author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},   booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},   pages={320--335},   year={2022} } |
| FLAN-T5 | @article{chung2022scaling,   title={Scaling instruction-finetuned language models},   author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},   journal={arXiv preprint arXiv:2210.11416},   year={2022} } |
| BLOOM | @article{workshop2022bloom,   title={Bloom: A 176b-parameter open-access multilingual language model},   author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},   journal={arXiv preprint arXiv:2211.05100},   year={2022} } |
| Galactica | @article{taylor2022galactica,   title={Galactica: A large language model for science},   author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},   journal={arXiv preprint arXiv:2211.09085},   year={2022} } |
| OPT-IML | @article{iyer2022opt,   title={Opt-iml: Scaling language model instruction meta learning through the lens of generalization},   author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},   journal={arXiv preprint arXiv:2212.12017},   year={2022} } |
| LLaMa | @article{touvron2023llama,   title={Llama: Open and efficient foundation language models},   author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},   journal={arXiv preprint arXiv:2302.13971},   year={2023} } |
| Baichuan 2 | @article{yang2023baichuan,   title={Baichuan 2: Open large-scale language models},   author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},   journal={arXiv preprint arXiv:2309.10305},   year={2023} } |
| QWEN | @article{bai2023qwen,   title={Qwen technical report},   author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},   journal={arXiv preprint arXiv:2309.16609},   year={2023} } |
| FLM | @article{li2023flm,   title={Flm-101b: An open llm and how to train it with $100 k budget},   author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others},   journal={arXiv preprint arXiv:2309.03852},   year={2023} } |
| Skywork | @article{wei2023skywork,   title={Skywork: A more open bilingual foundation model},   author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and others},   journal={arXiv preprint arXiv:2310.19341},   year={2023} } |
| Pythia | @inproceedings{biderman2023pythia,   title={Pythia: A suite for analyzing large language models across training and scaling},   author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},   booktitle={International Conference on Machine Learning},   pages={2397--2430},   year={2023},   organization={PMLR} } |
| CodeGen2 | @article{nijkamp2023codegen2,   title={Codegen2: Lessons for training llms on programming and natural languages},   author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},   journal={arXiv preprint arXiv:2305.02309},   year={2023} } |
| StarCoder | @article{li2023starcoder,   title={StarCoder: may the source be with you!},   author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},   journal={arXiv preprint arXiv:2305.06161},   year={2023} } |
| GShard | @inproceedings{ lepikhin2021gshard, title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen}, booktitle={International Conference on Learning Representations}, year={2021}, url={https://openreview.net/forum?id=qrwe7XHTmYb} } |
| ERNIE 3.0 | @article{sun2021ernie,   title={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},   author={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},   journal={arXiv preprint arXiv:2107.02137},   year={2021} } |
| Jurassic-1 | @article{lieber2021jurassic,   title={Jurassic-1: Technical details and evaluation},   author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},   journal={White Paper. AI21 Labs},   volume={1},   year={2021} } |
| HyperClova | @article{kim2021changes,   title={What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers},   author={Kim, Boseop and Kim, HyoungSeok and Lee, Sang-Woo and Lee, Gichang and Kwak, Donghyun and Jeon, Dong Hyeon and Park, Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and others},   journal={arXiv preprint arXiv:2109.04650},   year={2021} } |
| Yuan 1.0 | @article{wu2021yuan,   title={Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning},   author={Wu, Shaohua and Zhao, Xudong and Yu, Tong and Zhang, Rongguo and Shen, Chong and Liu, Hongli and Li, Feng and Zhu, Hong and Luo, Jiangang and Xu, Liang and others},   journal={arXiv preprint arXiv:2110.04725},   year={2021} } |
| FLAN | @inproceedings{ wei2022finetuned, title={Finetuned Language Models are Zero-Shot Learners}, author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le}, booktitle={International Conference on Learning Representations}, year={2022}, url={https://openreview.net/forum?id=gEZrGCozdqR} } |
| WebGPT | @article{nakano2021webgpt,   title={Webgpt: Browser-assisted question-answering with human feedback},   author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},   journal={arXiv preprint arXiv:2112.09332},   year={2021} } |
| Gopher | @article{rae2021scaling,   title={Scaling language models: Methods, analysis \& insights from training gopher},   author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},   journal={arXiv preprint arXiv:2112.11446},   year={2021} } |
| ERNIE-Code  | @misc{chai2023erniecode,       title={ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages},        author={Yekun Chai and Shuohuan Wang and Chao Pang and Yu Sun and Hao Tian and Hua Wu},       year={2023},       eprint={2212.06742},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CodeT5Mix | @misc{ wang2023codetmix, title={CodeT5Mix: A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation}, author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Junnan Li and Steven Hoi}, year={2023}, url={https://openreview.net/forum?id=VPCi3STZcaO} } |    
| CodeRL | @misc{le2022coderl,       title={CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},        author={Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven C. H. Hoi},       year={2022},       eprint={2207.01780},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| JuPyT5 | @misc{chandel2022training,       title={Training and Evaluating a Jupyter Notebook Data Science Assistant},        author={Shubham Chandel and Colin B. Clement and Guillermo Serrato and Neel Sundaresan},       year={2022},       eprint={2201.12901},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| PyMT5 | @misc{clement2020pymt5,       title={PyMT5: multi-mode translation of natural language and Python code with transformers},        author={Colin B. Clement and Dawn Drain and Jonathan Timcheck and Alexey Svyatkovskiy and Neel Sundaresan},       year={2020},       eprint={2010.03150},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| GLaM | @inproceedings{du2022glam,   title={Glam: Efficient scaling of language models with mixture-of-experts},   author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},   booktitle={International Conference on Machine Learning},   pages={5547--5569},   year={2022},   organization={PMLR} } |
| LaMDA | @article{thoppilan2022lamda,   title={Lamda: Language models for dialog applications},   author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},   journal={arXiv preprint arXiv:2201.08239},   year={2022} } |
| MT-NLG | @article{smith2022using,   title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},   author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},   journal={arXiv preprint arXiv:2201.11990},   year={2022} } |
| AlphaCode | @article{li2022competition,   title={Competition-level code generation with alphacode},   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},   journal={Science},   volume={378},   number={6624},   pages={1092--1097},   year={2022},   publisher={American Association for the Advancement of Science} } |
| InstructGPT | @article{ouyang2022training,   title={Training language models to follow instructions with human feedback},   author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},   journal={Advances in Neural Information Processing Systems},   volume={35},   pages={27730--27744},   year={2022} } |
| Chinchilla | @article{hoffmann2022training,   title={Training compute-optimal large language models},   author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},   journal={arXiv preprint arXiv:2203.15556},   year={2022} } |
| PaLM | @article{chowdhery2023palm,   title={Palm: Scaling language modeling with pathways},   author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},   journal={Journal of Machine Learning Research},   volume={24},   number={240},   pages={1--113},   year={2023} } |
| AlexaTM | @article{soltan2022alexatm,   title={Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model},   author={Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and others},   journal={arXiv preprint arXiv:2208.01448},   year={2022} } |
| PyCodeGPT | @inproceedings{ijcai2022p329,   title     = {CERT: Continual Pre-training on Sketches for Library-oriented Code Generation},   author    = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},   booktitle = {Proceedings of the Thirty-First International Joint Conference on                Artificial Intelligence, {IJCAI-22}},   publisher = {International Joint Conferences on Artificial Intelligence Organization},   editor    = {Lud De Raedt},   pages     = {2369--2375},   year      = {2022},   month     = {7},   note      = {Main Track},   doi       = {10.24963/ijcai.2022/329},   url       = {https://doi.org/10.24963/ijcai.2022/329}, } |
| Sparrow | @article{glaese2022improving,   title={Improving alignment of dialogue agents via targeted human judgements},   author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},   journal={arXiv preprint arXiv:2209.14375},   year={2022} } |
| WeLM | @article{su2022welm,   title={Welm: A well-read pre-trained language model for chinese},   author={Su, Hui and Zhou, Xiao and Yu, Houjin and Shen, Xiaoyu and Chen, Yuwen and Zhu, Zilin and Yu, Yang and Zhou, Jie},   journal={arXiv preprint arXiv:2209.10372},   year={2022} } |
| U-PaLM | @article{tay2022transcending,   title={Transcending scaling laws with 0.1\% extra compute},   author={Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q and So, David R and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and others},   journal={arXiv preprint arXiv:2210.11399},   year={2022} } |
| GPT-CC | https://github.com/CodedotAl/gpt-code-clippy/wiki |
| FIM | @article{Bavarian2022EfficientTO,   title={Efficient Training of Language Models to Fill in the Middle},   author={Mohammad Bavarian and Heewoo Jun and Nikolas A. Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},   journal={ArXiv},   year={2022},   volume={abs/2207.14255},   url={https://api.semanticscholar.org/CorpusID:251135268} } |
| Pangu-**Σ** | @article{ren2023pangu,   title={PanGu-$\{$$\backslash$Sigma$\}$: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},   author={Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and others},   journal={arXiv preprint arXiv:2303.10845},   year={2023} } |
| PaLM2 | @article{anil2023palm,   title={Palm 2 technical report},   author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},   journal={arXiv preprint arXiv:2305.10403},   year={2023} } |
| LLaMa 2 | @article{touvron2023llama,   title={Llama 2: Open foundation and fine-tuned chat models},   author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},   journal={arXiv preprint arXiv:2307.09288},   year={2023} } |
| Falcon | @article{penedo2023refinedweb,   title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},   author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},   journal={arXiv preprint arXiv:2306.01116},   year={2023} } |
| Mistral | @article{jiang2023mistral,   title={Mistral 7B},   author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},   journal={arXiv preprint arXiv:2310.06825},   year={2023} } |
| BERT | @inproceedings{Devlin2019BERTPO,   title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},   author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},   booktitle={North American Chapter of the Association for Computational Linguistics},   year={2019},   url={https://api.semanticscholar.org/CorpusID:52967399} } |
| WizardLM | @article{xu2023wizardlm,   title={Wizardlm: Empowering large language models to follow complex instructions},   author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},   journal={arXiv preprint arXiv:2304.12244},   year={2023} } |
| mBART | @article{liu2020multilingual,   title={Multilingual Denoising Pre-training for Neural Machine Translation},   author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},   journal={Transactions of the Association for Computational Linguistics},   volume={8},   pages={726--742},   year={2020} } |
| CodeBERT | @inproceedings{feng2020codebert,   title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},   author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},   booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},   pages={1536--1547},   year={2020} } |
| GPT-1 | @article{radford2018improving,   title={Improving language understanding by generative pre-training},   author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others} } |
| XLNet | @article{yang2019xlnet,   title={Xlnet: Generalized autoregressive pretraining for language understanding},   author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},   journal={Advances in neural information processing systems},   volume={32},   year={2019} } |
| GPT-J | @misc{gpt-j,   author = {Wang, Ben and Komatsuzaki, Aran},   title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},   howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},   year = 2021,   month = May } |
| GPT-C | @misc{svyatkovskiy2020intellicode,       title={IntelliCode Compose: Code Generation Using Transformer},        author={Alexey Svyatkovskiy and Shao Kun Deng and Shengyu Fu and Neel Sundaresan},       year={2020},       eprint={2005.08025},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CodeRetriever | @misc{li2022coderetriever,       title={CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search},        author={Xiaonan Li and Yeyun Gong and Yelong Shen and Xipeng Qiu and Hang Zhang and Bolun Yao and Weizhen Qi and Daxin Jiang and Weizhu Chen and Nan Duan},       year={2022},       eprint={2201.10866},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| PRCBERT | @inproceedings{prcbert, author = {Luo, Xianchang and Xue, Yinxing and Xing, Zhenchang and Sun, Jiamou}, title = {PRCBERT: Prompt Learning for Requirement Classification Using BERT-Based Pretrained Language Models}, year = {2023}, isbn = {9781450394758}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3551349.3560417}, doi = {10.1145/3551349.3560417}, abstract = {Software requirement classification is a longstanding and important problem in requirement engineering. Previous studies have applied various machine learning techniques for this problem, including Support Vector Machine (SVM) and decision trees. With the recent popularity of NLP technique, the state-of-the-art approach NoRBERT utilizes the pre-trained language model BERT and achieves a satisfactory performance. However, the dataset PROMISE used by the existing approaches for this problem consists of only hundreds of requirements that are outdated according to today’s technology and market trends. Besides, the NLP technique applied in these approaches might be obsolete. In this paper, we propose an approach of prompt learning for requirement classification using BERT-based pretrained language models (PRCBERT), which applies flexible prompt templates to achieve accurate requirements classification. Experiments conducted on two existing small-size requirement datasets (PROMISE and NFR-Review) and our collected large-scale requirement dataset NFR-SO prove that PRCBERT exhibits moderately better classification performance than NoRBERT and MLM-BERT (BERT with the standard prompt template). On the de-labeled NFR-Review and NFR-SO datasets, Trans_PRCBERT (the version of PRCBERT which is fine-tuned on PROMISE) is able to have a satisfactory zero-shot performance with 53.27\% and 72.96\% F1-score when enabling a self-learning strategy.}, booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering}, articleno = {75}, numpages = {13}, keywords = {requirement auto-labeling, prompt learning, self-learning strategy, requirement classification, zero-shot learning}, location = {, Rochester, MI, USA, }, series = {ASE '22} } |
| seBERT | @article{Trautsch2022PredictingIT,   title={Predicting Issue Types with seBERT},   author={Alexander Trautsch and Steffen Herbold},   journal={2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE)},   year={2022},   pages={37-39},   url={https://api.semanticscholar.org/CorpusID:248506056} } |
| TraceBERT | @misc{lin2021traceability,       title={Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},        author={Jinfeng Lin and Yalin Liu and Qingkai Zeng and Meng Jiang and Jane Cleland-Huang},       year={2021},       eprint={2102.04411},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| GraphCodeBERT | @misc{guo2021graphcodebert,       title={GraphCodeBERT: Pre-training Code Representations with Data Flow},        author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},       year={2021},       eprint={2009.08366},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| BERT Overflow | @inproceedings{tabassum-etal-2020-code,     title = "Code and Named Entity Recognition in {S}tack{O}verflow",     author = "Tabassum, Jeniya  and       Maddela, Mounica  and       Xu, Wei  and       Ritter, Alan",     editor = "Jurafsky, Dan  and       Chai, Joyce  and       Schluter, Natalie  and       Tetreault, Joel",     booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",     month = jul,     year = "2020",     address = "Online",     publisher = "Association for Computational Linguistics",     url = "https://aclanthology.org/2020.acl-main.443",     doi = "10.18653/v1/2020.acl-main.443",     pages = "4913--4926",} |
| ALBERT  | @misc{lan2020albert,       title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},        author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},       year={2020},       eprint={1909.11942},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| RoBERTa | @misc{liu2019roberta,       title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},        author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},       year={2019},       eprint={1907.11692},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CodeT5+ | @misc{wang2023codet5,       title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},        author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},       year={2023},       eprint={2305.07922},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| UnixCoder  | @misc{guo2022unixcoder,       title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},        author={Daya Guo and Shuai Lu and Nan Duan and Yanlin Wang and Ming Zhou and Jian Yin},       year={2022},       eprint={2203.03850},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CoText | @misc{phan2021cotext, title={CoTexT: Multi-task Learning with Code-Text Transformer}, author={Long Phan and Hieu Tran and Daniel Le and Hieu Nguyen and James Anibal and Alec Peltekian and Yanfang Ye}, year={2021}, eprint={2105.08645}, archivePrefix={arXiv},primaryClass={cs.AI} } |
| WizardCoder | @misc{luo2023wizardcoder,       title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},        author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},year={2023}, eprint={2306.08568},       archivePrefix={arXiv}, primaryClass={cs.CL} } |
| GPT-Neo | @software{black_2021_5297715,   author       = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and                   Biderman, Stella},   title        = {{GPT-Neo: Large Scale Autoregressive Language                     Modeling with Mesh-Tensorflow}},   month        = aug,   year         = 2021,   publisher    = {Zenodo},   version      = {1.0},   doi          = {10.5281/zenodo.5297715},   url          = {https://doi.org/10.5281/zenodo.5297715} } |
| CodeParrot | @book{tunstall2022nlp,   author    = {Lewis Tunstall and Leandro von Werra and Thomas Wolf},   title     = {Natural Language Processing with Transformers},   year      = {2022},   publisher = {O'Reilly Media, Inc.} } |
| SantaCoder | @misc{allal2023santacoder,       title={SantaCoder: don't reach for the stars!},        author={Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Munoz Ferrandis and Niklas Muennighoff and Mayank Mishra and Alex Gu and Manan Dey and Logesh Kumar Umapathi and Carolyn Jane Anderson and Yangtian Zi and Joel Lamy Poirier and Hailey Schoelkopf and Sergey Troshin and Dmitry Abulkhanov and Manuel Romero and Michael Lappert and Francesco De Toni and Bernardo García del Río and Qian Liu and Shamik Bose and Urvashi Bhattacharyya and Terry Yue Zhuo and Ian Yu and Paulo Villegas and Marco Zocca and Sourab Mangrulkar and David Lansky and Huu Nguyen and Danish Contractor and Luis Villa and Jia Li and Dzmitry Bahdanau and Yacine Jernite and Sean Hughes and Daniel Fried and Arjun Guha and Harm de Vries and Leandro von Werra},       year={2023},       eprint={2301.03988},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| PanGu-Coder | @misc{christopoulou2022pangucoder,       title={PanGu-Coder: Program Synthesis with Function-Level Language Modeling},        author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhongqi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li Yan and Pingyi Zhou and Xin Wang and Yuchi Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jiansheng Wei and Xin Jiang and Qianxiang Wang and Qun Liu},       year={2022},       eprint={2207.11280},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| PanGu-Coder-FT | @misc{christopoulou2022pangucoder,       title={PanGu-Coder: Program Synthesis with Function-Level Language Modeling},        author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhongqi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li Yan and Pingyi Zhou and Xin Wang and Yuchi Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jiansheng Wei and Xin Jiang and Qianxiang Wang and Qun Liu},       year={2022},       eprint={2207.11280},       archivePrefix={arXiv},       primaryClass={cs.LG} } |
| PolyCoder | @misc{xu2022systematic,       title={A Systematic Evaluation of Large Language Models of Code},        author={Frank F. Xu and Uri Alon and Graham Neubig and Vincent J. Hellendoorn},       year={2022},       eprint={2202.13169},       archivePrefix={arXiv},       primaryClass={cs.PL} } |      
| GPT-3.5 | https://platform.openai.com/docs/models/gpt-3-5 |
| PaLM-Coder | @misc{chowdhery2022palm,       title={PaLM: Scaling Language Modeling with Pathways},        author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},       year={2022},       eprint={2204.02311},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CodeGPT | @misc{lu2021codexglue,       title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},        author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},       year={2021},       eprint={2102.04664},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| CodeGPT-adapted | @misc{lu2021codexglue,       title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},        author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},       year={2021},       eprint={2102.04664},       archivePrefix={arXiv},       primaryClass={cs.SE} } |       
| PanGu-Coder2 | @misc{shen2023pangucoder2,       title={PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback},        author={Bo Shen and Jiaxin Zhang and Taihong Chen and Daoguang Zan and Bing Geng and An Fu and Muhan Zeng and Ailun Yu and Jichuan Ji and Jingyang Zhao and Yuenan Guo and Qianxiang Wang},       year={2023},       eprint={2307.14936},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| Code LLaMa | @misc{rozière2023code,       title={Code Llama: Open Foundation Models for Code},        author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},       year={2023},       eprint={2308.12950},       archivePrefix={arXiv},       primaryClass={cs.CL} } |
| CoditT5 | @misc{zhang2022coditt5,       title={CoditT5: Pretraining for Source Code and Natural Language Editing},        author={Jiyang Zhang and Sheena Panthaplackel and Pengyu Nie and Junyi Jessy Li and Milos Gligoric},       year={2022},       eprint={2208.05446},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| SPT-Code | @misc{niu2022sptcode,       title={SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations},        author={Changan Niu and Chuanyi Li and Vincent Ng and Jidong Ge and Liguo Huang and Bin Luo},       year={2022},       eprint={2201.01549},       archivePrefix={arXiv},       primaryClass={cs.SE} } |
| CuBERT | @misc{kanade2020learning,       title={Learning and Evaluating Contextual Embedding of Source Code},        author={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},       year={2020},       eprint={2001.00059},       archivePrefix={arXiv},       primaryClass={cs.SE} } |  
| T5-Learning | @article{Mastropaolo2021StudyingTU,   title={Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},   author={Antonio Mastropaolo and Simone Scalabrino and Nathan Cooper and David Nader-Palacio and Denys Poshyvanyk and Rocco Oliveto and Gabriele Bavota},   journal={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},   year={2021},   pages={336-347},   url={https://api.semanticscholar.org/CorpusID:231786586} } |
| Vicuna | @misc{vicuna2023,     title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},     url = {https://lmsys.org/blog/2023-03-30-vicuna/},     author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},     month = {March},     year = {2023} } |
| Stable Code | @misc{StableCodeCompleteAlpha4K,        url={[https://huggingface.co/stabilityai/stablecode-complete-alpha-3b-4k](https://huggingface.co/stabilityai/stablecode-complete-alpha-3b-4k)},        title={Stable Code Complete Alpha},        author={Adithyan, Reshinth and Phung, Duy and Cooper, Nathan and Pinnaparaju, Nikhil and Laforte, Christian} } |
| StableLM | @misc{StableLMAlphaV2Models, url={[https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2](https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2)}, title={StableLM Alpha v2 Models}, author={Tow, Jonathan}} |
| Stable LM Zephyr | https://stability.ai/news/stablelm-zephyr-3b-stability-llm |
| Stable Beluga | @misc{StableBelugaModels, url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, title={Stable Beluga models}, author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}} |
| Japanese StableLM | @misc{JapaneseStableLMBaseAlpha7B,        url={[https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b)},        title={Japanese StableLM Base Alpha 7B},        author={Lee, Meng and Nakamura, Fujiki and Shing, Makoto and McCann, Paul and Akiba, Takuya and Orii, Naoki} } |

> Written with [StackEdit](https://stackedit.io/).
